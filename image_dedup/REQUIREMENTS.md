# 画像重複整理ツール 要件定義書

## 1. 概要

### 1.1. プロジェクト名
Image Deduplicator (仮称)

### 1.2. 目的
本ツールは、指定されたディレクトリ内に存在する大量の画像ファイル（数万枚規模）を高速に分析し、視覚的に重複している画像を特定・整理することを目的とする。処理を「スキャン」「検出」「整理」の3つの独立したステップに分割することで、安全性と柔軟性の高いワークフローを提供し、ユーザーが安心してストレージの最適化と画像コレクションの管理を行えるように支援する。

### 1.3. ターゲットユーザー
大量のイラスト、写真、資料画像を扱うクリエイター、データ管理者、または個人のコレクションを整理したい一般ユーザー。

## 2. 機能要件

本ツールは、以下の3つのサブコマンドを持つコマンドラインインターフェース（CLI）アプリケーションとして実装する。

### 2.1. `scan` コマンド (ハッシュ生成)

#### 2.1.1. 目的
対象ディレクトリ内の画像ファイルをスキャンし、それぞれの知覚ハッシュを計算してデータベースファイルに保存する。

#### 2.1.2. 仕様
*   **入力**:
    *   必須引数: `[TARGET_DIRECTORY]` - スキャン対象のルートディレクトリ。
    *   オプション:
        *   `--output <PATH>`: ハッシュデータベースの出力ファイルパス。 (デフォルト: `hashes.json`)
        *   `--threads <NUMBER>`: 並列処理に使用するCPUスレッド数。 (デフォルト: システムで利用可能な全コア数)
        *   `--force`: 既存のデータベースファイルを警告なしで上書きする。
*   **処理ロジック**:
    1.  `TARGET_DIRECTORY`を再帰的に探索し、対象となる画像ファイル（拡張子: jpg, jpeg, png, gif, bmp, webp）をリストアップする。
    2.  リストアップされた各画像に対し、DCT（離散コサイン変換）ベースの知覚ハッシュを計算する。この処理は指定されたスレッド数で並列実行する。
    3.  計算結果を「絶対ファイルパス」をキー、「16進数文字列のハッシュ」をバリューとするKey-Valueストアとして、指定された`--output`ファイルにJSON形式で保存する。
*   **出力**:
    *   標準出力: 処理の進捗（例: プログレスバー）、処理済みファイル数、発見した画像総数、処理時間のサマリーを表示する。
    *   ファイル: ハッシュデータベースファイル (`hashes.json`など)。

### 2.2. `find-dups` コマンド (重複検出)

#### 2.2.1. 目的
`scan`コマンドで生成されたハッシュデータベースを基に、重複している画像のグループを特定し、その結果を重複リストファイルとして出力する。

#### 2.2.2. 仕様
*   **入力**:
    *   必須引数: `[HASH_DATABASE]` - `scan`で生成されたハッシュデータベースファイル。 (デフォルト: `hashes.json`)
    *   オプション:
        *   `--output <PATH>`: 重複リストの出力ファイルパス。 (デフォルト: `duplicates.json`)
        *   `--threshold <NUMBER>`: 2つのハッシュが重複していると見なすハミング距離の最大値。 (デフォルト: `5`)
*   **処理ロジック**:
    1.  ハッシュデータベースファイルを読み込む。
    2.  全ハッシュのペアを総当たりで比較し、ハミング距離が`--threshold`で指定された値以下のペアを特定する。
    3.  重複ペアを基に、重複している画像のグループを構築する。
    4.  各重複グループ内で、基準となる「オリジナル」画像を1つ決定する（基準: ファイルサイズが最も大きいものを優先）。残りを「重複」画像とする。
    5.  結果を、オリジナル画像のパスと、その重複画像のパスリストを含むオブジェクトの配列として、指定された`--output`ファイルにJSON形式で保存する。
*   **出力**:
    *   標準出力: 発見した重複グループ数、重複ファイル総数のサマリーを表示する。
    *   ファイル: 重複リストファイル (`duplicates.json`など)。

### 2.3. `process` コマンド (重複整理)

#### 2.3.1. 目的
`find-dups`コマンドで生成された重複リストに基づき、重複ファイルを実際に移動または削除する。

#### 2.3.2. 仕様
*   **入力**:
    *   必須引数: `[DUPLICATE_LIST]` - `find-dups`で生成された重複リストファイル。 (デフォルト: `duplicates.json`)
    *   オプション:
        *   `--action <ACTION>`: 重複ファイルに対して実行するアクション。`move`または`delete`から選択。 (デフォルト: `move`)
        *   `--dest <PATH>`: `--action move`の場合の、ファイルの移動先ディレクトリ。 (デフォルト: `./duplicates`)
        *   `--no-confirm`: 実行前の確認プロンプトをスキップする。
*   **処理ロジック**:
    1.  重複リストファイルを読み込む。
    2.  `--dest`で指定されたディレクトリが存在しない場合は作成する。
    3.  実行するアクション（移動/削除するファイル数など）の概要をユーザーに提示し、実行の確認を求める。`--no-confirm`フラグが指定されている場合は、この確認をスキップする。
    4.  ユーザーの承認後、リストに従って各重複ファイルを指定の場所に移動、または削除する。
*   **出力**:
    *   標準出力: 実行アクションのプレビュー、処理の進捗、完了メッセージを表示する。

## 3. 非機能要件

### 3.1. パフォーマンス
*   ハッシュ計算処理は、マルチコアCPUを最大限に活用し、高速に実行されること。
*   数万ファイル（〜10万ファイル）規模のデータに対しても、現実的な時間（数分〜数十分）で処理が完了すること。

### 3.2. 信頼性・安全性
*   ファイルシステムを変更する操作は`process`コマンドに限定し、デフォルトでユーザーに実行確認を求めることで、意図しないファイル操作を防ぐ。
*   処理中に画像ファイルが破損している、あるいは読み取り権限がないなどの問題が発生した場合、エラーを記録し、処理全体を停止させることなく、可能な限り続行する。

### 3.3. ユーザビリティ
*   各コマンドのオプションと引数は、ヘルプメッセージ (`--help`) で明確に説明されること。
*   処理中の進捗状況がユーザーに分かりやすくフィードバックされること。

## 4. 成果物
*   クロスプラットフォーム（Windows, macOS, Linux）で動作する、単一の実行可能バイナリ。
*   ソースコードリポジトリ（GitHubなど）。
*   ツールの使い方を説明した簡単なREADME.mdファイル。

## 5. 将来的な機能強化

### 5.1. GPUアクセラレーション
*   **目的**: `scan`コマンドにおけるハッシュ計算処理をGPUにオフロードすることで、大幅なパフォーマンス向上を図る。
*   **実装方針**:
    *   `wgpu`クレートを利用し、クロスプラットフォーム（Vulkan, Metal, DirectX）に対応したGPUコンピューティングを実装する。
    *   `scan`コマンドに`--gpu`フラグを追加する。
    *   ユーザーの環境で対応GPUが利用できない場合は、自動的にCPUベースの並列処理にフォールバックする仕組みを設ける。

### 5.2. データベースの更新機能
*   `scan`コマンドに`--update`フラグを追加し、既存のハッシュデータベースに新しい画像ファイルの情報のみを追記・更新できるようにする。これにより、定期的なスキャンの効率を向上させる。

### 5.3. オリジナル画像の選択基準の追加
*   `find-dups`コマンドに、重複グループ内のオリジナル画像を決定するための基準（例: `解像度が最も高い`, `更新日時が最も新しい/古い`）を選択できるオプションを追加する。
