# ç”»åƒé‡è¤‡æ¤œå‡ºãƒ„ãƒ¼ãƒ« - ä¸¦åˆ—å‡¦ç†å®Ÿè£…ã‚¿ã‚¹ã‚¯è¨ˆç”»æ›¸

## æ¦‚è¦

æœ¬æ›¸ã¯ã€ç”»åƒé‡è¤‡æ¤œå‡ºãƒ„ãƒ¼ãƒ«ã®ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®è©³ç´°ãªã‚¿ã‚¹ã‚¯è¨ˆç”»æ›¸ã§ã™ã€‚å„ã‚¿ã‚¹ã‚¯ã¯ `cargo test` ã¨ `cargo clippy` ã«ã‚ˆã‚‹æ¤œè¨¼ãŒå¯èƒ½ãªç²’åº¦ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€æ®µéšçš„ã‹ã¤å®‰å…¨ãªé–‹ç™ºã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## é–‹ç™ºæ–¹é‡

### å“è³ªä¿è¨¼
- **å„ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œ**: å¿…ãš `cargo test && cargo clippy` ã‚’å®Ÿè¡Œ
- **ã‚¨ãƒ©ãƒ¼ã‚¼ãƒ­æ–¹é‡**: è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ä¿®æ­£ã—ã¦ã‹ã‚‰æ¬¡ã‚¿ã‚¹ã‚¯ã«é€²ã‚€
- **æ®µéšçš„å®Ÿè£…**: å°ã•ãªå˜ä½ã§æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã€éƒ½åº¦å‹•ä½œç¢ºèª

### ä¸¦åˆ—é–‹ç™ºå¯¾å¿œ
- **ã‚¿ã‚¹ã‚¯ç‹¬ç«‹æ€§**: å¯èƒ½ãªé™ã‚Šå„ã‚¿ã‚¹ã‚¯ã‚’ç‹¬ç«‹ã—ã¦å®Ÿè£…å¯èƒ½ã«è¨­è¨ˆ
- **ãƒãƒ¼ã‚¸ç«¶åˆå›é¿**: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å˜ä½ã§ã‚¿ã‚¹ã‚¯ã‚’åˆ†å‰²
- **ãƒ†ã‚¹ãƒˆé§†å‹•**: ãƒ†ã‚¹ãƒˆã‚’å…ˆã«å®Ÿè£…ã—ã¦ã€ä»•æ§˜ã‚’æ˜ç¢ºåŒ–

## å®Ÿè£…ã‚¿ã‚¹ã‚¯è©³ç´°

### **Phase 1: åŸºç›¤ãƒˆãƒ¬ã‚¤ãƒˆå®Ÿè£…**
*æ¨å®šä½œæ¥­æ™‚é–“: 1æ—¥*

#### **Task 1.1: åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾©**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/mod.rs`

**å®Ÿè£…å†…å®¹**:
```rust
// å‡¦ç†æ™‚ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
#[derive(Debug, Clone, PartialEq)]
pub struct ProcessingMetadata {
    pub file_size: u64,
    pub processing_time_ms: u64,
    pub image_dimensions: (u32, u32),
    pub was_resized: bool,
}

// å‡¦ç†å…¨ä½“ã®ã‚µãƒãƒªãƒ¼
#[derive(Debug, PartialEq)]
pub struct ProcessingSummary {
    pub total_files: usize,
    pub processed_files: usize,
    pub error_count: usize,
    pub total_processing_time_ms: u64,
    pub average_time_per_file_ms: f64,
}

// å€‹åˆ¥å‡¦ç†ã®çµæœ
#[derive(Debug)]
pub enum ProcessingResult {
    Success {
        file_path: String,
        hash: String,
        metadata: ProcessingMetadata,
    },
    Error {
        file_path: String,
        error: String,
    },
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
- å„æ§‹é€ ä½“ã®ä½œæˆã¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹
- `ProcessingResult` ã®å„ãƒãƒªã‚¢ãƒ³ãƒˆä½œæˆ
- `Debug` ãƒˆãƒ¬ã‚¤ãƒˆã®å‹•ä½œç¢ºèª

**æˆåŠŸåŸºæº–**: 
- `cargo test` ã§æ–°è¦ãƒ†ã‚¹ãƒˆå…¨ã¦ãƒ‘ã‚¹
- `cargo clippy` ã§è­¦å‘Šãªã—

---

#### **Task 1.2: ProcessingConfig ãƒˆãƒ¬ã‚¤ãƒˆå®šç¾©**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/mod.rs`

**å®Ÿè£…å†…å®¹**:
```rust
/// ä¸¦åˆ—å‡¦ç†ã®è¨­å®šã‚’æŠ½è±¡åŒ–
pub trait ProcessingConfig: Send + Sync {
    /// åŒæ™‚å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã®æœ€å¤§æ•°
    fn max_concurrent_tasks(&self) -> usize;
    
    /// ãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    fn channel_buffer_size(&self) -> usize;
    
    /// ãƒãƒƒãƒå‡¦ç†ã®ã‚µã‚¤ã‚º
    fn batch_size(&self) -> usize;
    
    /// é€²æ—å ±å‘Šã®æœ‰åŠ¹/ç„¡åŠ¹
    fn enable_progress_reporting(&self) -> bool;
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    struct MockConfig {
        max_concurrent: usize,
        buffer_size: usize,
        batch_size: usize,
        enable_progress: bool,
    }
    
    impl ProcessingConfig for MockConfig {
        fn max_concurrent_tasks(&self) -> usize { self.max_concurrent }
        fn channel_buffer_size(&self) -> usize { self.buffer_size }
        fn batch_size(&self) -> usize { self.batch_size }
        fn enable_progress_reporting(&self) -> bool { self.enable_progress }
    }
    
    #[test]
    fn test_processing_config_implementation() {
        let config = MockConfig {
            max_concurrent: 4,
            buffer_size: 100,
            batch_size: 50,
            enable_progress: true,
        };
        
        assert_eq!(config.max_concurrent_tasks(), 4);
        assert_eq!(config.channel_buffer_size(), 100);
        assert_eq!(config.batch_size(), 50);
        assert!(config.enable_progress_reporting());
    }
}
```

**æˆåŠŸåŸºæº–**:
- ãƒˆãƒ¬ã‚¤ãƒˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ
- Mockãƒ†ã‚¹ãƒˆå®Ÿè£…ã¨ãƒ‘ã‚¹

---

#### **Task 1.3: ProgressReporter ãƒˆãƒ¬ã‚¤ãƒˆå®šç¾©**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/mod.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use async_trait::async_trait;

/// é€²æ—å ±å‘Šã®æŠ½è±¡åŒ–
#[async_trait]
pub trait ProgressReporter: Send + Sync {
    /// å‡¦ç†é–‹å§‹æ™‚ã®å ±å‘Š
    async fn report_started(&self, total_files: usize);
    
    /// é€²æ—æ›´æ–°ã®å ±å‘Š
    async fn report_progress(&self, completed: usize, total: usize);
    
    /// ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å ±å‘Š
    async fn report_error(&self, file_path: &str, error: &str);
    
    /// å‡¦ç†å®Œäº†æ™‚ã®å ±å‘Š
    async fn report_completed(&self, total_processed: usize, total_errors: usize);
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::{Arc, Mutex};

    #[derive(Default)]
    struct MockReporter {
        calls: Arc<Mutex<Vec<String>>>,
    }
    
    #[async_trait]
    impl ProgressReporter for MockReporter {
        async fn report_started(&self, total_files: usize) {
            self.calls.lock().unwrap().push(format!("started:{}", total_files));
        }
        
        async fn report_progress(&self, completed: usize, total: usize) {
            self.calls.lock().unwrap().push(format!("progress:{}:{}", completed, total));
        }
        
        async fn report_error(&self, file_path: &str, error: &str) {
            self.calls.lock().unwrap().push(format!("error:{}:{}", file_path, error));
        }
        
        async fn report_completed(&self, total_processed: usize, total_errors: usize) {
            self.calls.lock().unwrap().push(format!("completed:{}:{}", total_processed, total_errors));
        }
    }
    
    #[tokio::test]
    async fn test_progress_reporter() {
        let reporter = MockReporter::default();
        
        reporter.report_started(100).await;
        reporter.report_progress(50, 100).await;
        reporter.report_error("/path/test.jpg", "load failed").await;
        reporter.report_completed(99, 1).await;
        
        let calls = reporter.calls.lock().unwrap();
        assert_eq!(calls.len(), 4);
        assert_eq!(calls[0], "started:100");
        assert_eq!(calls[1], "progress:50:100");
        assert_eq!(calls[2], "error:/path/test.jpg:load failed");
        assert_eq!(calls[3], "completed:99:1");
    }
}
```

**æˆåŠŸåŸºæº–**:
- éåŒæœŸãƒˆãƒ¬ã‚¤ãƒˆã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ
- Mockå®Ÿè£…ã§ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 1.4: HashPersistence ãƒˆãƒ¬ã‚¤ãƒˆå®šç¾©**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/mod.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use anyhow::Result;

/// å‡¦ç†çµæœã®æ°¸ç¶šåŒ–æŠ½è±¡åŒ–
#[async_trait]
pub trait HashPersistence: Send + Sync {
    /// å˜ä¸€ãƒãƒƒã‚·ãƒ¥ã®ä¿å­˜
    async fn store_hash(
        &self,
        file_path: &str,
        hash: &str,
        metadata: &ProcessingMetadata,
    ) -> Result<()>;
    
    /// ãƒãƒƒãƒã§ã®ãƒãƒƒã‚·ãƒ¥ä¿å­˜
    async fn store_batch(
        &self,
        results: &[(String, String, ProcessingMetadata)],
    ) -> Result<()>;
    
    /// æ°¸ç¶šåŒ–ã®å®Œäº†å‡¦ç†
    async fn finalize(&self) -> Result<()>;
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;
    use std::sync::{Arc, Mutex};

    #[derive(Default)]
    struct MockPersistence {
        storage: Arc<Mutex<HashMap<String, (String, ProcessingMetadata)>>>,
        finalized: Arc<Mutex<bool>>,
    }
    
    #[async_trait]
    impl HashPersistence for MockPersistence {
        async fn store_hash(
            &self,
            file_path: &str,
            hash: &str,
            metadata: &ProcessingMetadata,
        ) -> Result<()> {
            self.storage
                .lock()
                .unwrap()
                .insert(file_path.to_string(), (hash.to_string(), metadata.clone()));
            Ok(())
        }
        
        async fn store_batch(
            &self,
            results: &[(String, String, ProcessingMetadata)],
        ) -> Result<()> {
            let mut storage = self.storage.lock().unwrap();
            for (path, hash, metadata) in results {
                storage.insert(path.clone(), (hash.clone(), metadata.clone()));
            }
            Ok(())
        }
        
        async fn finalize(&self) -> Result<()> {
            *self.finalized.lock().unwrap() = true;
            Ok(())
        }
    }
    
    #[tokio::test]
    async fn test_hash_persistence() {
        let persistence = MockPersistence::default();
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        // å˜ä¸€ä¿å­˜ãƒ†ã‚¹ãƒˆ
        persistence.store_hash("/test1.jpg", "hash1", &metadata).await.unwrap();
        
        // ãƒãƒƒãƒä¿å­˜ãƒ†ã‚¹ãƒˆ
        let batch = vec![
            ("/test2.jpg".to_string(), "hash2".to_string(), metadata.clone()),
            ("/test3.jpg".to_string(), "hash3".to_string(), metadata.clone()),
        ];
        persistence.store_batch(&batch).await.unwrap();
        
        // å®Œäº†å‡¦ç†ãƒ†ã‚¹ãƒˆ
        persistence.finalize().await.unwrap();
        
        let storage = persistence.storage.lock().unwrap();
        assert_eq!(storage.len(), 3);
        assert!(storage.contains_key("/test1.jpg"));
        assert!(storage.contains_key("/test2.jpg"));
        assert!(storage.contains_key("/test3.jpg"));
        
        assert!(*persistence.finalized.lock().unwrap());
    }
}
```

**æˆåŠŸåŸºæº–**:
- ãƒˆãƒ¬ã‚¤ãƒˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ
- Mockå®Ÿè£…ã§ã®CRUDãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 1.5: ParallelProcessor ãƒˆãƒ¬ã‚¤ãƒˆå®šç¾©**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/mod.rs`

**å®Ÿè£…å†…å®¹**:
```rust
/// ä¸¦åˆ—å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®æŠ½è±¡åŒ–
#[async_trait]
pub trait ParallelProcessor: Send + Sync {
    type Config: ProcessingConfig;
    type Reporter: ProgressReporter;
    type Persistence: HashPersistence;

    /// ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
    async fn process_directory(
        &self,
        path: &str,
        config: &Self::Config,
        reporter: &Self::Reporter,
        persistence: &Self::Persistence,
    ) -> Result<ProcessingSummary>;
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç¢ºèªç”¨ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…
    struct DummyProcessor;
    
    #[async_trait]
    impl ParallelProcessor for DummyProcessor {
        type Config = MockConfig;
        type Reporter = MockReporter;
        type Persistence = MockPersistence;
        
        async fn process_directory(
            &self,
            _path: &str,
            _config: &Self::Config,
            _reporter: &Self::Reporter,
            _persistence: &Self::Persistence,
        ) -> Result<ProcessingSummary> {
            Ok(ProcessingSummary {
                total_files: 0,
                processed_files: 0,
                error_count: 0,
                total_processing_time_ms: 0,
                average_time_per_file_ms: 0.0,
            })
        }
    }
    
    #[tokio::test]
    async fn test_parallel_processor_trait() {
        let processor = DummyProcessor;
        let config = MockConfig {
            max_concurrent: 4,
            buffer_size: 100,
            batch_size: 50,
            enable_progress: true,
        };
        let reporter = MockReporter::default();
        let persistence = MockPersistence::default();
        
        let result = processor
            .process_directory("/test", &config, &reporter, &persistence)
            .await
            .unwrap();
            
        assert_eq!(result.total_files, 0);
        assert_eq!(result.processed_files, 0);
    }
}
```

**æˆåŠŸåŸºæº–**:
- é–¢é€£å‹ã‚’ä½¿ã£ãŸãƒˆãƒ¬ã‚¤ãƒˆã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ
- ãƒ€ãƒŸãƒ¼å®Ÿè£…ã§ã®åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

### **Phase 2: åŸºæœ¬å…·è±¡å®Ÿè£…**
*æ¨å®šä½œæ¥­æ™‚é–“: 1æ—¥*

#### **Task 2.1: DefaultProcessingConfigå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/implementations.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use super::ProcessingConfig;

/// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šå®Ÿè£…
#[derive(Debug, Clone)]
pub struct DefaultProcessingConfig {
    max_concurrent: usize,
    buffer_size: usize,
    batch_size: usize,
    enable_progress: bool,
}

impl DefaultProcessingConfig {
    pub fn new() -> Self {
        Self::default()
    }
    
    pub fn with_max_concurrent(mut self, max_concurrent: usize) -> Self {
        self.max_concurrent = max_concurrent;
        self
    }
    
    pub fn with_buffer_size(mut self, buffer_size: usize) -> Self {
        self.buffer_size = buffer_size;
        self
    }
    
    pub fn with_batch_size(mut self, batch_size: usize) -> Self {
        self.batch_size = batch_size;
        self
    }
    
    pub fn with_progress_reporting(mut self, enable: bool) -> Self {
        self.enable_progress = enable;
        self
    }
}

impl Default for DefaultProcessingConfig {
    fn default() -> Self {
        Self {
            max_concurrent: num_cpus::get().max(1) * 2,
            buffer_size: 100,
            batch_size: 50,
            enable_progress: true,
        }
    }
}

impl ProcessingConfig for DefaultProcessingConfig {
    fn max_concurrent_tasks(&self) -> usize {
        self.max_concurrent
    }
    
    fn channel_buffer_size(&self) -> usize {
        self.buffer_size
    }
    
    fn batch_size(&self) -> usize {
        self.batch_size
    }
    
    fn enable_progress_reporting(&self) -> bool {
        self.enable_progress
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_default_processing_config() {
        let config = DefaultProcessingConfig::default();
        
        assert!(config.max_concurrent_tasks() > 0);
        assert_eq!(config.channel_buffer_size(), 100);
        assert_eq!(config.batch_size(), 50);
        assert!(config.enable_progress_reporting());
    }
    
    #[test]
    fn test_processing_config_builder() {
        let config = DefaultProcessingConfig::new()
            .with_max_concurrent(8)
            .with_buffer_size(200)
            .with_batch_size(100)
            .with_progress_reporting(false);
            
        assert_eq!(config.max_concurrent_tasks(), 8);
        assert_eq!(config.channel_buffer_size(), 200);
        assert_eq!(config.batch_size(), 100);
        assert!(!config.enable_progress_reporting());
    }
}
```

**æˆåŠŸåŸºæº–**:
- `num_cpus` ã‚¯ãƒ¬ãƒ¼ãƒˆã®ä¾å­˜é–¢ä¿‚è¿½åŠ 
- è¨­å®šã®å–å¾—ãƒ»å¤‰æ›´ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 2.2: ConsoleProgressReporterå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/implementations.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use super::ProgressReporter;
use async_trait::async_trait;

/// ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã«ã‚ˆã‚‹é€²æ—å ±å‘Šå®Ÿè£…
#[derive(Debug, Default)]
pub struct ConsoleProgressReporter {
    quiet: bool,
}

impl ConsoleProgressReporter {
    pub fn new() -> Self {
        Self::default()
    }
    
    pub fn quiet() -> Self {
        Self { quiet: true }
    }
}

#[async_trait]
impl ProgressReporter for ConsoleProgressReporter {
    async fn report_started(&self, total_files: usize) {
        if !self.quiet {
            println!("ğŸš€ Starting processing {} files...", total_files);
        }
    }
    
    async fn report_progress(&self, completed: usize, total: usize) {
        if !self.quiet && (completed % 100 == 0 || completed == total) {
            let percentage = (completed as f64 / total as f64) * 100.0;
            println!("ğŸ“Š Progress: {}/{} ({:.1}%)", completed, total, percentage);
        }
    }
    
    async fn report_error(&self, file_path: &str, error: &str) {
        if !self.quiet {
            eprintln!("âŒ Error processing {}: {}", file_path, error);
        }
    }
    
    async fn report_completed(&self, total_processed: usize, total_errors: usize) {
        if !self.quiet {
            println!("âœ… Completed! Processed: {}, Errors: {}", total_processed, total_errors);
        }
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_console_progress_reporter() {
        // å‡ºåŠ›ã‚­ãƒ£ãƒ—ãƒãƒ£ã¯è¤‡é›‘ãªãŸã‚ã€åŸºæœ¬çš„ãªå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆã®ã¿
        let reporter = ConsoleProgressReporter::quiet(); // quiet modeã§ãƒ†ã‚¹ãƒˆ
        
        reporter.report_started(100).await;
        reporter.report_progress(50, 100).await;
        reporter.report_error("/test.jpg", "test error").await;
        reporter.report_completed(99, 1).await;
        
        // ãƒ‘ãƒ‹ãƒƒã‚¯ã—ãªã‘ã‚Œã°OK
        assert!(true);
    }
    
    #[tokio::test]
    async fn test_console_progress_reporter_creation() {
        let reporter1 = ConsoleProgressReporter::new();
        let reporter2 = ConsoleProgressReporter::quiet();
        
        assert!(!reporter1.quiet);
        assert!(reporter2.quiet);
    }
}
```

**æˆåŠŸåŸºæº–**:
- ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›å®Ÿè£…ã®å®Œäº†
- åŸºæœ¬çš„ãªå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 2.3: NoOpProgressReporterå®Ÿè£…ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/implementations.rs`

**å®Ÿè£…å†…å®¹**:
```rust
/// ä½•ã‚‚ã—ãªã„é€²æ—å ±å‘Šå®Ÿè£…ï¼ˆãƒ†ã‚¹ãƒˆãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ï¼‰
#[derive(Debug, Default)]
pub struct NoOpProgressReporter;

impl NoOpProgressReporter {
    pub fn new() -> Self {
        Self::default()
    }
}

#[async_trait]
impl ProgressReporter for NoOpProgressReporter {
    async fn report_started(&self, _total_files: usize) {
        // ä½•ã‚‚ã—ãªã„
    }
    
    async fn report_progress(&self, _completed: usize, _total: usize) {
        // ä½•ã‚‚ã—ãªã„
    }
    
    async fn report_error(&self, _file_path: &str, _error: &str) {
        // ä½•ã‚‚ã—ãªã„
    }
    
    async fn report_completed(&self, _total_processed: usize, _total_errors: usize) {
        // ä½•ã‚‚ã—ãªã„
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_noop_progress_reporter() {
        let reporter = NoOpProgressReporter::new();
        
        // å…¨ã¦ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ã‚‚ãƒ‘ãƒ‹ãƒƒã‚¯ã—ãªã„
        reporter.report_started(100).await;
        reporter.report_progress(50, 100).await;
        reporter.report_error("/test.jpg", "test error").await;
        reporter.report_completed(99, 1).await;
        
        assert!(true);
    }
}
```

**æˆåŠŸåŸºæº–**:
- ç©ºå®Ÿè£…ã®å®Œäº†
- åŸºæœ¬å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 2.4: MemoryHashPersistenceå®Ÿè£…ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/implementations.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use super::{HashPersistence, ProcessingMetadata};
use anyhow::Result;
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

/// ãƒ¡ãƒ¢ãƒªå†…ä¿å­˜ã®æ°¸ç¶šåŒ–å®Ÿè£…ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
#[derive(Debug, Clone)]
pub struct MemoryHashPersistence {
    storage: Arc<Mutex<HashMap<String, (String, ProcessingMetadata)>>>,
    finalized: Arc<Mutex<bool>>,
}

impl Default for MemoryHashPersistence {
    fn default() -> Self {
        Self::new()
    }
}

impl MemoryHashPersistence {
    pub fn new() -> Self {
        Self {
            storage: Arc::new(Mutex::new(HashMap::new())),
            finalized: Arc::new(Mutex::new(false)),
        }
    }
    
    /// ãƒ†ã‚¹ãƒˆç”¨ï¼šä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    pub fn get_stored_data(&self) -> HashMap<String, (String, ProcessingMetadata)> {
        self.storage.lock().unwrap().clone()
    }
    
    /// ãƒ†ã‚¹ãƒˆç”¨ï¼šå®Œäº†çŠ¶æ…‹ã‚’ç¢ºèª
    pub fn is_finalized(&self) -> bool {
        *self.finalized.lock().unwrap()
    }
    
    /// ãƒ†ã‚¹ãƒˆç”¨ï¼šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
    pub fn clear(&self) {
        self.storage.lock().unwrap().clear();
        *self.finalized.lock().unwrap() = false;
    }
}

#[async_trait]
impl HashPersistence for MemoryHashPersistence {
    async fn store_hash(
        &self,
        file_path: &str,
        hash: &str,
        metadata: &ProcessingMetadata,
    ) -> Result<()> {
        self.storage
            .lock()
            .unwrap()
            .insert(file_path.to_string(), (hash.to_string(), metadata.clone()));
        Ok(())
    }
    
    async fn store_batch(
        &self,
        results: &[(String, String, ProcessingMetadata)],
    ) -> Result<()> {
        let mut storage = self.storage.lock().unwrap();
        for (path, hash, metadata) in results {
            storage.insert(path.clone(), (hash.clone(), metadata.clone()));
        }
        Ok(())
    }
    
    async fn finalize(&self) -> Result<()> {
        *self.finalized.lock().unwrap() = true;
        Ok(())
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_memory_hash_persistence() {
        let persistence = MemoryHashPersistence::new();
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        // å˜ä¸€ä¿å­˜ãƒ†ã‚¹ãƒˆ
        persistence.store_hash("/test1.jpg", "hash1", &metadata).await.unwrap();
        
        let stored = persistence.get_stored_data();
        assert_eq!(stored.len(), 1);
        assert_eq!(stored["/test1.jpg"].0, "hash1");
        assert_eq!(stored["/test1.jpg"].1, metadata);
        
        // ãƒãƒƒãƒä¿å­˜ãƒ†ã‚¹ãƒˆ
        let batch = vec![
            ("/test2.jpg".to_string(), "hash2".to_string(), metadata.clone()),
            ("/test3.jpg".to_string(), "hash3".to_string(), metadata.clone()),
        ];
        persistence.store_batch(&batch).await.unwrap();
        
        let stored = persistence.get_stored_data();
        assert_eq!(stored.len(), 3);
        
        // å®Œäº†å‡¦ç†ãƒ†ã‚¹ãƒˆ
        assert!(!persistence.is_finalized());
        persistence.finalize().await.unwrap();
        assert!(persistence.is_finalized());
    }
    
    #[tokio::test]
    async fn test_memory_persistence_clear() {
        let persistence = MemoryHashPersistence::new();
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        persistence.store_hash("/test.jpg", "hash", &metadata).await.unwrap();
        persistence.finalize().await.unwrap();
        
        assert_eq!(persistence.get_stored_data().len(), 1);
        assert!(persistence.is_finalized());
        
        persistence.clear();
        
        assert_eq!(persistence.get_stored_data().len(), 0);
        assert!(!persistence.is_finalized());
    }
}
```

**æˆåŠŸåŸºæº–**:
- ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã®å®Ÿè£…å®Œäº†
- CRUDæ“ä½œã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

### **Phase 3: ã‚³ã‚¢å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…**
*æ¨å®šä½œæ¥­æ™‚é–“: 0.5æ—¥*

#### **Task 3.1: ParallelProcessingEngineæ§‹é€ ä½“å®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/engine.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use crate::{
    image_loader::ImageLoaderBackend,
    perceptual_hash::PerceptualHashBackend,
    storage::StorageBackend,
};
use super::{ParallelProcessor, ProcessingSummary};
use anyhow::Result;
use async_trait::async_trait;
use std::sync::Arc;

/// ä¾å­˜æ€§æ³¨å…¥ã«ã‚ˆã‚‹ã‚³ã‚¢å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
pub struct ParallelProcessingEngine<L, H, S> {
    loader: Arc<L>,
    hasher: Arc<H>,
    storage: Arc<S>,
}

impl<L, H, S> ParallelProcessingEngine<L, H, S>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
    S: StorageBackend + 'static,
{
    /// ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
    pub fn new(loader: L, hasher: H, storage: S) -> Self {
        Self {
            loader: Arc::new(loader),
            hasher: Arc::new(hasher),
            storage: Arc::new(storage),
        }
    }

    /// ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæ—¢å­˜ã®Appã‹ã‚‰æ§‹ç¯‰ï¼‰
    pub fn from_app(app: crate::App<L, H, S>) -> Self {
        Self::new(app.loader, app.hasher, app.storage)
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::image_loader::standard::StandardImageLoader;
    use crate::perceptual_hash::dct_hash::DCTHasher;
    use crate::storage::local::LocalStorageBackend;

    #[test]
    fn test_parallel_processing_engine_new() {
        let loader = StandardImageLoader::new();
        let hasher = DCTHasher::new(8);
        let storage = LocalStorageBackend::new();
        
        let engine = ParallelProcessingEngine::new(loader, hasher, storage);
        
        // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒé€šã‚Œã°æˆåŠŸ
        assert!(true);
    }
    
    #[test]
    fn test_parallel_processing_engine_from_app() {
        let app = crate::App::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let engine = ParallelProcessingEngine::from_app(app);
        
        // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒé€šã‚Œã°æˆåŠŸ
        assert!(true);
    }
}
```

**æˆåŠŸåŸºæº–**:
- æ§‹é€ ä½“å®šç¾©ã¨ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®å®Ÿè£…å®Œäº†
- åŸºæœ¬çš„ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 3.2: ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹æ©Ÿèƒ½å®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/engine.rs`

**å®Ÿè£…å†…å®¹**:
```rust
impl<L, H, S> ParallelProcessingEngine<L, H, S>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
    S: StorageBackend + 'static,
{
    /// ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹
    async fn discover_image_files(&self, path: &str) -> Result<Vec<String>> {
        let items = self.storage.list_items(path).await?;
        
        let mut image_files = Vec::new();
        for item in items {
            if !item.is_directory && self.storage.is_image_file(&item) {
                image_files.push(item.id);
            }
        }
        
        image_files.sort(); // ä¸€è²«ã—ãŸé †åºã§å‡¦ç†
        Ok(image_files)
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;

    #[tokio::test]
    async fn test_discover_image_files() {
        // ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        let temp_dir = TempDir::new().unwrap();
        let temp_path = temp_dir.path().to_str().unwrap();
        
        // ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        fs::write(temp_dir.path().join("test1.jpg"), b"fake jpg content").unwrap();
        fs::write(temp_dir.path().join("test2.png"), b"fake png content").unwrap();
        fs::write(temp_dir.path().join("not_image.txt"), b"text content").unwrap();
        
        // ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹å®Ÿè¡Œ
        let files = engine.discover_image_files(temp_path).await.unwrap();
        
        // ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãŒç™ºè¦‹ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert_eq!(files.len(), 2);
        assert!(files.iter().any(|f| f.ends_with("test1.jpg")));
        assert!(files.iter().any(|f| f.ends_with("test2.png")));
        assert!(!files.iter().any(|f| f.ends_with("not_image.txt")));
    }
    
    #[tokio::test]
    async fn test_discover_empty_directory() {
        let temp_dir = TempDir::new().unwrap();
        let temp_path = temp_dir.path().to_str().unwrap();
        
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let files = engine.discover_image_files(temp_path).await.unwrap();
        assert_eq!(files.len(), 0);
    }
}
```

**ä¾å­˜é–¢ä¿‚è¿½åŠ **: `tempfile = "3.8"` ã‚’ `[dev-dependencies]` ã«è¿½åŠ 

**æˆåŠŸåŸºæº–**:
- ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…å®Œäº†
- ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã®ç™ºè¦‹ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 3.3: å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ©Ÿèƒ½å®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/engine.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use super::{ProcessingMetadata, ProcessingResult};
use std::time::Instant;

impl<L, H, S> ParallelProcessingEngine<L, H, S>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
    S: StorageBackend + 'static,
{
    /// å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    async fn process_single_file(
        loader: &L,
        hasher: &H,
        file_path: &str,
        _worker_id: usize,
    ) -> ProcessingResult {
        let start_time = Instant::now();
        
        let result = async {
            // ç”»åƒèª­ã¿è¾¼ã¿
            let load_result = loader.load_from_path(file_path).await?;
            
            // ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
            let hash_result = hasher.generate_hash(&load_result.image).await?;
            
            // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            let metadata = ProcessingMetadata {
                file_size: load_result.file_size,
                processing_time_ms: start_time.elapsed().as_millis() as u64,
                image_dimensions: (load_result.image.width(), load_result.image.height()),
                was_resized: load_result.was_resized,
            };
            
            Result::<(String, ProcessingMetadata)>::Ok((hash_result.to_hex(), metadata))
        }.await;
        
        match result {
            Ok((hash, metadata)) => ProcessingResult::Success {
                file_path: file_path.to_string(),
                hash,
                metadata,
            },
            Err(error) => ProcessingResult::Error {
                file_path: file_path.to_string(),
                error: error.to_string(),
            },
        }
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;

    #[tokio::test]
    async fn test_process_single_file_success() {
        // 1x1ã®æœ€å°PNGãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ‰åŠ¹ãªç”»åƒãƒ‡ãƒ¼ã‚¿ï¼‰
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        let test_file = temp_dir.path().join("test.png");
        fs::write(&test_file, &png_data).unwrap();
        
        let loader = StandardImageLoader::new();
        let hasher = DCTHasher::new(8);
        
        let result = ParallelProcessingEngine::process_single_file(
            &loader,
            &hasher,
            test_file.to_str().unwrap(),
            0,
        ).await;
        
        match result {
            ProcessingResult::Success { file_path, hash, metadata } => {
                assert!(file_path.ends_with("test.png"));
                assert!(!hash.is_empty());
                assert_eq!(metadata.image_dimensions, (1, 1));
                assert!(metadata.processing_time_ms > 0);
            }
            ProcessingResult::Error { .. } => panic!("Expected success"),
        }
    }
    
    #[tokio::test]
    async fn test_process_single_file_error() {
        let temp_dir = TempDir::new().unwrap();
        let invalid_file = temp_dir.path().join("invalid.jpg");
        fs::write(&invalid_file, b"not a valid image").unwrap();
        
        let loader = StandardImageLoader::new();
        let hasher = DCTHasher::new(8);
        
        let result = ParallelProcessingEngine::process_single_file(
            &loader,
            &hasher,
            invalid_file.to_str().unwrap(),
            0,
        ).await;
        
        match result {
            ProcessingResult::Success { .. } => panic!("Expected error"),
            ProcessingResult::Error { file_path, error } => {
                assert!(file_path.ends_with("invalid.jpg"));
                assert!(!error.is_empty());
            }
        }
    }
    
    #[tokio::test]
    async fn test_process_nonexistent_file() {
        let loader = StandardImageLoader::new();
        let hasher = DCTHasher::new(8);
        
        let result = ParallelProcessingEngine::process_single_file(
            &loader,
            &hasher,
            "/nonexistent/file.jpg",
            0,
        ).await;
        
        match result {
            ProcessingResult::Success { .. } => panic!("Expected error"),
            ProcessingResult::Error { file_path, error } => {
                assert_eq!(file_path, "/nonexistent/file.jpg");
                assert!(!error.is_empty());
            }
        }
    }
}
```

**æˆåŠŸåŸºæº–**:
- å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…å®Œäº†
- æˆåŠŸãƒ»ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åé›†ã®ç¢ºèª

---

---

### **Phase 4: Producer-Consumer ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…**
*æ¨å®šä½œæ¥­æ™‚é–“: 1.5æ—¥*

#### **Task 4.1: ProcessingPipelineæ§‹é€ ä½“å®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/pipeline.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use crate::{
    image_loader::ImageLoaderBackend,
    perceptual_hash::PerceptualHashBackend,
};
use super::{ProcessingConfig, ProgressReporter, HashPersistence, ProcessingResult, ProcessingSummary};
use anyhow::Result;
use tokio::sync::mpsc;
use std::sync::Arc;

/// è²¬ä»»ãŒæ˜ç¢ºã«åˆ†é›¢ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pub struct ProcessingPipeline<L, H> {
    loader: Arc<L>,
    hasher: Arc<H>,
}

impl<L, H> ProcessingPipeline<L, H>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
{
    /// æ–°ã—ã„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pub fn new(loader: Arc<L>, hasher: Arc<H>) -> Self {
        Self { loader, hasher }
    }

    /// ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å‡¦ç†
    pub async fn execute<C, R, P>(
        &self,
        files: Vec<String>,
        config: &C,
        reporter: &R,
        persistence: &P,
    ) -> Result<ProcessingSummary>
    where
        C: ProcessingConfig,
        R: ProgressReporter,
        P: HashPersistence,
    {
        // Producer-Consumerãƒãƒ£ãƒ³ãƒãƒ«æ§‹ç¯‰
        let (work_tx, work_rx) = mpsc::channel::<String>(config.channel_buffer_size());
        let (result_tx, result_rx) = mpsc::channel::<ProcessingResult>(config.channel_buffer_size());
        
        // åŒæœŸãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–
        let semaphore = Arc::new(tokio::sync::Semaphore::new(config.max_concurrent_tasks()));
        let processed_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let error_count = Arc::new(tokio::sync::RwLock::new(0usize));
        
        // ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…ï¼ˆå¾Œç¶šã‚¿ã‚¹ã‚¯ã§å®Ÿè£…ï¼‰
        Ok(ProcessingSummary {
            total_files: files.len(),
            processed_files: 0,
            error_count: 0,
            total_processing_time_ms: 0,
            average_time_per_file_ms: 0.0,
        })
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::image_loader::standard::StandardImageLoader;
    use crate::perceptual_hash::dct_hash::DCTHasher;
    use crate::processing::{
        DefaultProcessingConfig, NoOpProgressReporter, MemoryHashPersistence
    };

    #[tokio::test]
    async fn test_processing_pipeline_creation() {
        let loader = Arc::new(StandardImageLoader::new());
        let hasher = Arc::new(DCTHasher::new(8));
        
        let pipeline = ProcessingPipeline::new(loader, hasher);
        
        // åŸºæœ¬çš„ãªä½œæˆãƒ†ã‚¹ãƒˆ
        assert!(true);
    }
    
    #[tokio::test]
    async fn test_processing_pipeline_empty_files() {
        let pipeline = ProcessingPipeline::new(
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
        );
        
        let config = DefaultProcessingConfig::default();
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let result = pipeline.execute(
            vec![],
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(result.total_files, 0);
        assert_eq!(result.processed_files, 0);
        assert_eq!(result.error_count, 0);
    }
}
```

**æˆåŠŸåŸºæº–**:
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŸºæœ¬æ§‹é€ ã®å®Ÿè£…å®Œäº†
- ç©ºãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã§ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 4.2: Producerå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/pipeline.rs`

**å®Ÿè£…å†…å®¹**:
```rust
impl<L, H> ProcessingPipeline<L, H>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
{
    /// Producer: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é…ä¿¡
    fn spawn_producer(
        files: Vec<String>,
        work_tx: mpsc::Sender<String>,
    ) -> tokio::task::JoinHandle<Result<()>> {
        tokio::spawn(async move {
            for file_path in files {
                if let Err(_) = work_tx.send(file_path).await {
                    // ãƒãƒ£ãƒ³ãƒãƒ«ãŒé–‰ã˜ã‚‰ã‚ŒãŸå ´åˆã¯æ­£å¸¸çµ‚äº†
                    break;
                }
            }
            // work_txã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãƒãƒ£ãƒ³ãƒãƒ«çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
            Ok(())
        })
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc;
    use tokio::time::{timeout, Duration};

    #[tokio::test]
    async fn test_producer_sends_all_files() {
        let files = vec![
            "/test1.jpg".to_string(),
            "/test2.png".to_string(),
            "/test3.gif".to_string(),
        ];
        
        let (work_tx, mut work_rx) = mpsc::channel::<String>(10);
        
        // Producerèµ·å‹•
        let producer_handle = ProcessingPipeline::<StandardImageLoader, DCTHasher>::spawn_producer(
            files.clone(),
            work_tx,
        );
        
        // å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ä¿¡
        let mut received = Vec::new();
        while let Ok(Some(file_path)) = timeout(Duration::from_millis(100), work_rx.recv()).await {
            received.push(file_path);
        }
        
        // Producerå®Œäº†ç¢ºèª
        producer_handle.await.unwrap().unwrap();
        
        // é€ä¿¡å†…å®¹ç¢ºèª
        assert_eq!(received.len(), 3);
        assert_eq!(received, files);
    }
    
    #[tokio::test]
    async fn test_producer_empty_files() {
        let files: Vec<String> = vec![];
        let (work_tx, mut work_rx) = mpsc::channel::<String>(10);
        
        let producer_handle = ProcessingPipeline::<StandardImageLoader, DCTHasher>::spawn_producer(
            files,
            work_tx,
        );
        
        // ãƒãƒ£ãƒ³ãƒãƒ«ãŒå³åº§ã«é–‰ã˜ã‚‹ã“ã¨ã‚’ç¢ºèª
        let received = timeout(Duration::from_millis(100), work_rx.recv()).await;
        assert!(received.is_err() || received.unwrap().is_none());
        
        producer_handle.await.unwrap().unwrap();
    }
    
    #[tokio::test]
    async fn test_producer_channel_closed_early() {
        let files = vec!["/test1.jpg".to_string(), "/test2.jpg".to_string()];
        let (work_tx, work_rx) = mpsc::channel::<String>(1);
        
        // å—ä¿¡å´ã‚’å³åº§ã«é–‰ã˜ã‚‹
        drop(work_rx);
        
        let producer_handle = ProcessingPipeline::<StandardImageLoader, DCTHasher>::spawn_producer(
            files,
            work_tx,
        );
        
        // Producerã¯ã‚¨ãƒ©ãƒ¼ãªãçµ‚äº†ã™ã¹ã
        producer_handle.await.unwrap().unwrap();
    }
}
```

**æˆåŠŸåŸºæº–**:
- ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…å®Œäº†
- æ­£å¸¸ã‚±ãƒ¼ã‚¹ãƒ»ç•°å¸¸ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 4.3: å˜ä¸€Consumerå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/pipeline.rs`

**å®Ÿè£…å†…å®¹**:
```rust
impl<L, H> ProcessingPipeline<L, H>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
{
    /// å˜ä¸€Consumerãƒ¯ãƒ¼ã‚«ãƒ¼
    fn spawn_single_consumer(
        worker_id: usize,
        loader: Arc<L>,
        hasher: Arc<H>,
        work_rx: Arc<tokio::sync::Mutex<mpsc::Receiver<String>>>,
        result_tx: mpsc::Sender<ProcessingResult>,
        semaphore: Arc<tokio::sync::Semaphore>,
    ) -> tokio::task::JoinHandle<Result<()>> {
        tokio::spawn(async move {
            loop {
                // æ¬¡ã®ä½œæ¥­ã‚’å–å¾—
                let file_path = {
                    let mut rx = work_rx.lock().await;
                    match rx.recv().await {
                        Some(path) => path,
                        None => break, // ãƒãƒ£ãƒ³ãƒãƒ«çµ‚äº†
                    }
                };
                
                // ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°åˆ¶å¾¡
                let _permit = semaphore.acquire().await
                    .map_err(|e| anyhow::anyhow!("Semaphore error: {}", e))?;
                
                // å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
                let result = Self::process_single_file(
                    &loader,
                    &hasher,
                    &file_path,
                    worker_id,
                ).await;
                
                // çµæœé€ä¿¡
                if let Err(_) = result_tx.send(result).await {
                    // çµæœãƒãƒ£ãƒ³ãƒãƒ«ãŒé–‰ã˜ã‚‰ã‚ŒãŸå ´åˆã¯çµ‚äº†
                    break;
                }
            }
            Ok(())
        })
    }

    /// å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆengine.rsã‹ã‚‰ç§»å‹•ï¼‰
    async fn process_single_file(
        loader: &L,
        hasher: &H,
        file_path: &str,
        _worker_id: usize,
    ) -> ProcessingResult {
        use std::time::Instant;
        use super::ProcessingMetadata;
        
        let start_time = Instant::now();
        
        let result = async {
            let load_result = loader.load_from_path(file_path).await?;
            let hash_result = hasher.generate_hash(&load_result.image).await?;
            
            let metadata = ProcessingMetadata {
                file_size: load_result.file_size,
                processing_time_ms: start_time.elapsed().as_millis() as u64,
                image_dimensions: (load_result.image.width(), load_result.image.height()),
                was_resized: load_result.was_resized,
            };
            
            Result::<(String, ProcessingMetadata)>::Ok((hash_result.to_hex(), metadata))
        }.await;
        
        match result {
            Ok((hash, metadata)) => ProcessingResult::Success {
                file_path: file_path.to_string(),
                hash,
                metadata,
            },
            Err(error) => ProcessingResult::Error {
                file_path: file_path.to_string(),
                error: error.to_string(),
            },
        }
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;

    #[tokio::test]
    async fn test_single_consumer_processes_files() {
        // ãƒ†ã‚¹ãƒˆç”¨ç”»åƒä½œæˆ
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        let test_file = temp_dir.path().join("test.png");
        fs::write(&test_file, &png_data).unwrap();
        
        // ãƒãƒ£ãƒ³ãƒãƒ«ä½œæˆ
        let (work_tx, work_rx) = mpsc::channel::<String>(10);
        let (result_tx, mut result_rx) = mpsc::channel::<ProcessingResult>(10);
        let work_rx = Arc::new(tokio::sync::Mutex::new(work_rx));
        let semaphore = Arc::new(tokio::sync::Semaphore::new(1));
        
        // ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
        let worker_handle = ProcessingPipeline::spawn_single_consumer(
            0,
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
            work_rx,
            result_tx,
            semaphore,
        );
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹é€ä¿¡
        work_tx.send(test_file.to_str().unwrap().to_string()).await.unwrap();
        drop(work_tx); // ãƒãƒ£ãƒ³ãƒãƒ«çµ‚äº†
        
        // çµæœå—ä¿¡
        let result = result_rx.recv().await.unwrap();
        
        // ãƒ¯ãƒ¼ã‚«ãƒ¼å®Œäº†ç¢ºèª
        worker_handle.await.unwrap().unwrap();
        
        // çµæœç¢ºèª
        match result {
            ProcessingResult::Success { file_path, hash, metadata } => {
                assert!(file_path.ends_with("test.png"));
                assert!(!hash.is_empty());
                assert_eq!(metadata.image_dimensions, (1, 1));
            }
            ProcessingResult::Error { .. } => panic!("Expected success"),
        }
    }
    
    #[tokio::test]
    async fn test_single_consumer_handles_errors() {
        let temp_dir = TempDir::new().unwrap();
        let invalid_file = temp_dir.path().join("invalid.jpg");
        fs::write(&invalid_file, b"not a valid image").unwrap();
        
        let (work_tx, work_rx) = mpsc::channel::<String>(10);
        let (result_tx, mut result_rx) = mpsc::channel::<ProcessingResult>(10);
        let work_rx = Arc::new(tokio::sync::Mutex::new(work_rx));
        let semaphore = Arc::new(tokio::sync::Semaphore::new(1));
        
        let worker_handle = ProcessingPipeline::spawn_single_consumer(
            0,
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
            work_rx,
            result_tx,
            semaphore,
        );
        
        work_tx.send(invalid_file.to_str().unwrap().to_string()).await.unwrap();
        drop(work_tx);
        
        let result = result_rx.recv().await.unwrap();
        worker_handle.await.unwrap().unwrap();
        
        match result {
            ProcessingResult::Success { .. } => panic!("Expected error"),
            ProcessingResult::Error { file_path, error } => {
                assert!(file_path.ends_with("invalid.jpg"));
                assert!(!error.is_empty());
            }
        }
    }
}
```

**æˆåŠŸåŸºæº–**:
- å˜ä¸€ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…å®Œäº†
- æˆåŠŸãƒ»ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹åˆ¶å¾¡ã®ç¢ºèª

---

#### **Task 4.4: Consumer Poolå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/pipeline.rs`

**å®Ÿè£…å†…å®¹**:
```rust
impl<L, H> ProcessingPipeline<L, H>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
{
    /// Consumers: ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
    fn spawn_consumers(
        &self,
        work_rx: mpsc::Receiver<String>,
        result_tx: mpsc::Sender<ProcessingResult>,
        semaphore: Arc<tokio::sync::Semaphore>,
        worker_count: usize,
    ) -> Vec<tokio::task::JoinHandle<Result<()>>> {
        let work_rx = Arc::new(tokio::sync::Mutex::new(work_rx));
        let mut handles = Vec::new();
        
        for worker_id in 0..worker_count {
            let handle = Self::spawn_single_consumer(
                worker_id,
                self.loader.clone(),
                self.hasher.clone(),
                work_rx.clone(),
                result_tx.clone(),
                semaphore.clone(),
            );
            handles.push(handle);
        }
        
        handles
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashSet;
    use tokio::time::{timeout, Duration};

    #[tokio::test]
    async fn test_consumer_pool_processes_multiple_files() {
        // è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆç”¨ç”»åƒä½œæˆ
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        let mut test_files = Vec::new();
        
        for i in 0..5 {
            let test_file = temp_dir.path().join(format!("test{}.png", i));
            fs::write(&test_file, &png_data).unwrap();
            test_files.push(test_file.to_str().unwrap().to_string());
        }
        
        // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
        let pipeline = ProcessingPipeline::new(
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
        );
        
        // ãƒãƒ£ãƒ³ãƒãƒ«ä½œæˆ
        let (work_tx, work_rx) = mpsc::channel::<String>(10);
        let (result_tx, mut result_rx) = mpsc::channel::<ProcessingResult>(10);
        let semaphore = Arc::new(tokio::sync::Semaphore::new(3));
        
        // Consumer poolèµ·å‹•
        let worker_handles = pipeline.spawn_consumers(
            work_rx,
            result_tx,
            semaphore,
            3, // 3ã¤ã®ãƒ¯ãƒ¼ã‚«ãƒ¼
        );
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹é€ä¿¡
        for file_path in &test_files {
            work_tx.send(file_path.clone()).await.unwrap();
        }
        drop(work_tx); // ãƒãƒ£ãƒ³ãƒãƒ«çµ‚äº†
        
        // çµæœåé›†
        let mut results = Vec::new();
        while results.len() < test_files.len() {
            if let Ok(Some(result)) = timeout(Duration::from_secs(5), result_rx.recv()).await {
                results.push(result);
            } else {
                break;
            }
        }
        
        // ãƒ¯ãƒ¼ã‚«ãƒ¼å®Œäº†ç¢ºèª
        for handle in worker_handles {
            handle.await.unwrap().unwrap();
        }
        
        // çµæœç¢ºèª
        assert_eq!(results.len(), 5);
        let processed_files: HashSet<String> = results.iter().map(|r| match r {
            ProcessingResult::Success { file_path, .. } => file_path.clone(),
            ProcessingResult::Error { file_path, .. } => file_path.clone(),
        }).collect();
        
        for file_path in &test_files {
            assert!(processed_files.iter().any(|p| p.contains(&format!("test{}.png", 
                file_path.split("test").nth(1).unwrap().split('.').next().unwrap()))));
        }
    }
    
    #[tokio::test]
    async fn test_consumer_pool_with_mixed_results() {
        let temp_dir = TempDir::new().unwrap();
        
        // æœ‰åŠ¹ãªç”»åƒ
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let valid_file = temp_dir.path().join("valid.png");
        fs::write(&valid_file, &png_data).unwrap();
        
        let invalid_file = temp_dir.path().join("invalid.jpg");
        fs::write(&invalid_file, b"not a valid image").unwrap();
        
        let pipeline = ProcessingPipeline::new(
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
        );
        
        let (work_tx, work_rx) = mpsc::channel::<String>(10);
        let (result_tx, mut result_rx) = mpsc::channel::<ProcessingResult>(10);
        let semaphore = Arc::new(tokio::sync::Semaphore::new(2));
        
        let worker_handles = pipeline.spawn_consumers(work_rx, result_tx, semaphore, 2);
        
        work_tx.send(valid_file.to_str().unwrap().to_string()).await.unwrap();
        work_tx.send(invalid_file.to_str().unwrap().to_string()).await.unwrap();
        drop(work_tx);
        
        let mut success_count = 0;
        let mut error_count = 0;
        
        for _ in 0..2 {
            if let Ok(Some(result)) = timeout(Duration::from_secs(5), result_rx.recv()).await {
                match result {
                    ProcessingResult::Success { .. } => success_count += 1,
                    ProcessingResult::Error { .. } => error_count += 1,
                }
            }
        }
        
        for handle in worker_handles {
            handle.await.unwrap().unwrap();
        }
        
        assert_eq!(success_count, 1);
        assert_eq!(error_count, 1);
    }
}
```

**æˆåŠŸåŸºæº–**:
- è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…å®Œäº†
- ä¸¦åˆ—å‡¦ç†ã§ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- æˆåŠŸãƒ»å¤±æ•—æ··åœ¨ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 4.5: Result Collectorå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/pipeline.rs`

**å®Ÿè£…å†…å®¹**:
```rust
impl<L, H> ProcessingPipeline<L, H>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
{
    /// Collector: çµæœåé›†ã¨æ°¸ç¶šåŒ–
    fn spawn_result_collector<R, P>(
        mut result_rx: mpsc::Receiver<ProcessingResult>,
        total_files: usize,
        processed_count: Arc<tokio::sync::RwLock<usize>>,
        error_count: Arc<tokio::sync::RwLock<usize>>,
        reporter: &R,
        persistence: &P,
        batch_size: usize,
    ) -> tokio::task::JoinHandle<Result<()>>
    where
        R: ProgressReporter + 'static,
        P: HashPersistence + 'static,
    {
        let reporter = Arc::new(reporter);
        let persistence = Arc::new(persistence);
        
        tokio::spawn(async move {
            let mut batch = Vec::with_capacity(batch_size);
            let mut completed = 0;
            let mut errors = 0;
            
            while let Some(result) = result_rx.recv().await {
                match result {
                    ProcessingResult::Success { file_path, hash, metadata } => {
                        batch.push((file_path, hash, metadata));
                        completed += 1;
                        
                        // ãƒãƒƒãƒæ°¸ç¶šåŒ–
                        if batch.len() >= batch_size {
                            persistence.store_batch(&batch).await?;
                            batch.clear();
                        }
                    }
                    ProcessingResult::Error { file_path, error } => {
                        reporter.report_error(&file_path, &error).await;
                        errors += 1;
                    }
                }
                
                // é€²æ—å ±å‘Š
                reporter.report_progress(completed + errors, total_files).await;
            }
            
            // æ®‹ã‚Šãƒãƒƒãƒã®æ°¸ç¶šåŒ–
            if !batch.is_empty() {
                persistence.store_batch(&batch).await?;
            }
            
            // ã‚«ã‚¦ãƒ³ã‚¿æ›´æ–°
            *processed_count.write().await = completed;
            *error_count.write().await = errors;
            
            Ok(())
        })
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::processing::{ProcessingMetadata, MemoryHashPersistence, NoOpProgressReporter};
    use tokio::sync::mpsc;

    #[tokio::test]
    async fn test_result_collector_processes_success_results() {
        let (result_tx, result_rx) = mpsc::channel::<ProcessingResult>(10);
        let processed_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let error_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let collector_handle = ProcessingPipeline::<StandardImageLoader, DCTHasher>::spawn_result_collector(
            result_rx,
            3,
            processed_count.clone(),
            error_count.clone(),
            &reporter,
            &persistence,
            2, // ãƒãƒƒãƒã‚µã‚¤ã‚º
        );
        
        // æˆåŠŸçµæœã‚’é€ä¿¡
        for i in 0..3 {
            let metadata = ProcessingMetadata {
                file_size: 1024,
                processing_time_ms: 100,
                image_dimensions: (512, 512),
                was_resized: false,
            };
            
            result_tx.send(ProcessingResult::Success {
                file_path: format!("/test{}.jpg", i),
                hash: format!("hash{}", i),
                metadata,
            }).await.unwrap();
        }
        
        drop(result_tx); // ãƒãƒ£ãƒ³ãƒãƒ«çµ‚äº†
        
        // ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼å®Œäº†ç¢ºèª
        collector_handle.await.unwrap().unwrap();
        
        // çµæœç¢ºèª
        assert_eq!(*processed_count.read().await, 3);
        assert_eq!(*error_count.read().await, 0);
        
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 3);
        assert!(stored_data.contains_key("/test0.jpg"));
        assert!(stored_data.contains_key("/test1.jpg"));
        assert!(stored_data.contains_key("/test2.jpg"));
    }
    
    #[tokio::test]
    async fn test_result_collector_processes_mixed_results() {
        let (result_tx, result_rx) = mpsc::channel::<ProcessingResult>(10);
        let processed_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let error_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let collector_handle = ProcessingPipeline::<StandardImageLoader, DCTHasher>::spawn_result_collector(
            result_rx,
            4,
            processed_count.clone(),
            error_count.clone(),
            &reporter,
            &persistence,
            10, // å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
        );
        
        // æˆåŠŸçµæœ
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        result_tx.send(ProcessingResult::Success {
            file_path: "/success1.jpg".to_string(),
            hash: "hash1".to_string(),
            metadata: metadata.clone(),
        }).await.unwrap();
        
        result_tx.send(ProcessingResult::Success {
            file_path: "/success2.jpg".to_string(),
            hash: "hash2".to_string(),
            metadata,
        }).await.unwrap();
        
        // ã‚¨ãƒ©ãƒ¼çµæœ
        result_tx.send(ProcessingResult::Error {
            file_path: "/error1.jpg".to_string(),
            error: "load failed".to_string(),
        }).await.unwrap();
        
        result_tx.send(ProcessingResult::Error {
            file_path: "/error2.jpg".to_string(),
            error: "invalid format".to_string(),
        }).await.unwrap();
        
        drop(result_tx);
        collector_handle.await.unwrap().unwrap();
        
        assert_eq!(*processed_count.read().await, 2);
        assert_eq!(*error_count.read().await, 2);
        
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 2);
        assert!(stored_data.contains_key("/success1.jpg"));
        assert!(stored_data.contains_key("/success2.jpg"));
    }
    
    #[tokio::test]
    async fn test_result_collector_batching() {
        let (result_tx, result_rx) = mpsc::channel::<ProcessingResult>(10);
        let processed_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let error_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let collector_handle = ProcessingPipeline::<StandardImageLoader, DCTHasher>::spawn_result_collector(
            result_rx,
            5,
            processed_count.clone(),
            error_count.clone(),
            &reporter,
            &persistence,
            2, // ãƒãƒƒãƒã‚µã‚¤ã‚º2
        );
        
        // 5ã¤ã®æˆåŠŸçµæœï¼ˆ2+2+1ã®ãƒãƒƒãƒã«åˆ†ã‹ã‚Œã‚‹ã¯ãšï¼‰
        for i in 0..5 {
            let metadata = ProcessingMetadata {
                file_size: 1024,
                processing_time_ms: 100,
                image_dimensions: (512, 512),
                was_resized: false,
            };
            
            result_tx.send(ProcessingResult::Success {
                file_path: format!("/test{}.jpg", i),
                hash: format!("hash{}", i),
                metadata,
            }).await.unwrap();
        }
        
        drop(result_tx);
        collector_handle.await.unwrap().unwrap();
        
        assert_eq!(*processed_count.read().await, 5);
        assert_eq!(*error_count.read().await, 0);
        
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 5);
    }
}
```

**æˆåŠŸåŸºæº–**:
- çµæœåé›†ã¨æ°¸ç¶šåŒ–ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…å®Œäº†
- ãƒãƒƒãƒå‡¦ç†ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- æˆåŠŸãƒ»å¤±æ•—æ··åœ¨å‡¦ç†ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 4.6: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/pipeline.rs`

**å®Ÿè£…å†…å®¹**:
```rust
impl<L, H> ProcessingPipeline<L, H>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
{
    /// å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pub async fn execute<C, R, P>(
        &self,
        files: Vec<String>,
        config: &C,
        reporter: &R,
        persistence: &P,
    ) -> Result<ProcessingSummary>
    where
        C: ProcessingConfig,
        R: ProgressReporter,
        P: HashPersistence,
    {
        let total_files = files.len();
        
        if total_files == 0 {
            return Ok(ProcessingSummary {
                total_files: 0,
                processed_files: 0,
                error_count: 0,
                total_processing_time_ms: 0,
                average_time_per_file_ms: 0.0,
            });
        }
        
        // Producer-Consumerãƒãƒ£ãƒ³ãƒãƒ«æ§‹ç¯‰
        let (work_tx, work_rx) = mpsc::channel::<String>(config.channel_buffer_size());
        let (result_tx, result_rx) = mpsc::channel::<ProcessingResult>(config.channel_buffer_size());
        
        // åŒæœŸãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–
        let semaphore = Arc::new(tokio::sync::Semaphore::new(config.max_concurrent_tasks()));
        let processed_count = Arc::new(tokio::sync::RwLock::new(0usize));
        let error_count = Arc::new(tokio::sync::RwLock::new(0usize));
        
        // 3ã¤ã®ç‹¬ç«‹ã—ãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆèµ·å‹•
        let producer_handle = Self::spawn_producer(files, work_tx);
        
        let consumer_handles = self.spawn_consumers(
            work_rx,
            result_tx,
            semaphore,
            config.max_concurrent_tasks(),
        );
        
        let collector_handle = Self::spawn_result_collector(
            result_rx,
            total_files,
            processed_count.clone(),
            error_count.clone(),
            reporter,
            persistence,
            config.batch_size(),
        );
        
        // å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å®Œäº†å¾…æ©Ÿ
        producer_handle.await??;
        
        // å…¨Consumerã®å®Œäº†å¾…æ©Ÿ
        for handle in consumer_handles {
            handle.await??;
        }
        
        // Result Collectorã®å®Œäº†å¾…æ©Ÿ
        collector_handle.await??;
        
        // çµæœã‚µãƒãƒªãƒ¼æ§‹ç¯‰
        let processed = *processed_count.read().await;
        let errors = *error_count.read().await;
        
        Ok(ProcessingSummary {
            total_files,
            processed_files: processed,
            error_count: errors,
            total_processing_time_ms: 0, // å‘¼ã³å‡ºã—å…ƒã§è¨ˆç®—
            average_time_per_file_ms: 0.0,
        })
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;

    #[tokio::test]
    async fn test_pipeline_end_to_end() {
        // è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆç”¨ç”»åƒä½œæˆ
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        let mut test_files = Vec::new();
        
        // æœ‰åŠ¹ãªç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        for i in 0..3 {
            let test_file = temp_dir.path().join(format!("valid{}.png", i));
            fs::write(&test_file, &png_data).unwrap();
            test_files.push(test_file.to_str().unwrap().to_string());
        }
        
        // ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        let invalid_file = temp_dir.path().join("invalid.jpg");
        fs::write(&invalid_file, b"not a valid image").unwrap();
        test_files.push(invalid_file.to_str().unwrap().to_string());
        
        // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        let pipeline = ProcessingPipeline::new(
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
        );
        
        let config = DefaultProcessingConfig::default()
            .with_max_concurrent(2)
            .with_batch_size(2);
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let summary = pipeline.execute(
            test_files,
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        // çµæœç¢ºèª
        assert_eq!(summary.total_files, 4);
        assert_eq!(summary.processed_files, 3); // æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«3ã¤
        assert_eq!(summary.error_count, 1); // ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«1ã¤
        
        // æ°¸ç¶šåŒ–ç¢ºèª
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 3);
        
        for i in 0..3 {
            assert!(stored_data.iter().any(|(path, _)| path.contains(&format!("valid{}.png", i))));
        }
    }
    
    #[tokio::test]
    async fn test_pipeline_empty_files() {
        let pipeline = ProcessingPipeline::new(
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
        );
        
        let config = DefaultProcessingConfig::default();
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let summary = pipeline.execute(
            vec![],
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(summary.total_files, 0);
        assert_eq!(summary.processed_files, 0);
        assert_eq!(summary.error_count, 0);
    }
    
    #[tokio::test]
    async fn test_pipeline_with_high_concurrency() {
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        let mut test_files = Vec::new();
        
        // 10å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        for i in 0..10 {
            let test_file = temp_dir.path().join(format!("test{}.png", i));
            fs::write(&test_file, &png_data).unwrap();
            test_files.push(test_file.to_str().unwrap().to_string());
        }
        
        let pipeline = ProcessingPipeline::new(
            Arc::new(StandardImageLoader::new()),
            Arc::new(DCTHasher::new(8)),
        );
        
        let config = DefaultProcessingConfig::default()
            .with_max_concurrent(8) // é«˜ã„ä¸¦åˆ—åº¦
            .with_batch_size(3);
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let summary = pipeline.execute(
            test_files,
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(summary.total_files, 10);
        assert_eq!(summary.processed_files, 10);
        assert_eq!(summary.error_count, 0);
        
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 10);
    }
}
```

**æˆåŠŸåŸºæº–**:
- å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã®å®Ÿè£…å®Œäº†
- ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- é«˜ä¸¦åˆ—åº¦ã§ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

### **Phase 5: ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆã¨ParallelProcessorå®Ÿè£…**
*æ¨å®šä½œæ¥­æ™‚é–“: 1æ—¥*

#### **Task 5.1: ParallelProcessor traitå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/engine.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use super::{ParallelProcessor, DefaultProcessingConfig, NoOpProgressReporter, MemoryHashPersistence};
use super::pipeline::ProcessingPipeline;
use std::time::Instant;

#[async_trait]
impl<L, H, S> ParallelProcessor for ParallelProcessingEngine<L, H, S>
where
    L: ImageLoaderBackend + 'static,
    H: PerceptualHashBackend + 'static,
    S: StorageBackend + 'static,
{
    type Config = dyn ProcessingConfig;
    type Reporter = dyn ProgressReporter;
    type Persistence = dyn HashPersistence;

    async fn process_directory(
        &self,
        path: &str,
        config: &Self::Config,
        reporter: &Self::Reporter,
        persistence: &Self::Persistence,
    ) -> Result<ProcessingSummary> {
        let start_time = Instant::now();
        
        // 1. ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ãƒ•ã‚§ãƒ¼ã‚º
        let files = self.discover_image_files(path).await?;
        let total_files = files.len();
        
        if config.enable_progress_reporting() {
            reporter.report_started(total_files).await;
        }
        
        // 2. Producer-Consumerãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
        let pipeline = ProcessingPipeline::new(
            self.loader.clone(),
            self.hasher.clone(),
        );
        
        // 3. ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        let mut summary = pipeline.execute(files, config, reporter, persistence).await?;
        
        // 4. ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆæ¸¬å®Œäº†
        let total_time = start_time.elapsed().as_millis() as u64;
        summary.total_processing_time_ms = total_time;
        
        if summary.processed_files > 0 {
            summary.average_time_per_file_ms = total_time as f64 / summary.processed_files as f64;
        }
        
        if config.enable_progress_reporting() {
            reporter.report_completed(summary.processed_files, summary.error_count).await;
        }
        
        // 5. æ°¸ç¶šåŒ–å®Œäº†å‡¦ç†
        persistence.finalize().await?;
        
        Ok(summary)
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;

    #[tokio::test]
    async fn test_parallel_processor_trait_implementation() {
        // ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ç”»åƒä½œæˆ
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        
        // ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        for i in 0..3 {
            let test_file = temp_dir.path().join(format!("test{}.png", i));
            fs::write(&test_file, &png_data).unwrap();
        }
        
        // éç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆç„¡è¦–ã•ã‚Œã‚‹ã¯ãšï¼‰
        fs::write(temp_dir.path().join("readme.txt"), b"text content").unwrap();
        
        // ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        // è¨­å®šã¨ãƒ¬ãƒãƒ¼ã‚¿ãƒ¼ä½œæˆ
        let config = DefaultProcessingConfig::default().with_max_concurrent(2);
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        // å‡¦ç†å®Ÿè¡Œ
        let summary = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        // çµæœç¢ºèª
        assert_eq!(summary.total_files, 3); // ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        assert_eq!(summary.processed_files, 3);
        assert_eq!(summary.error_count, 0);
        assert!(summary.total_processing_time_ms > 0);
        assert!(summary.average_time_per_file_ms > 0.0);
        
        // æ°¸ç¶šåŒ–ç¢ºèª
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 3);
        assert!(persistence.is_finalized());
        
        for i in 0..3 {
            assert!(stored_data.iter().any(|(path, _)| path.contains(&format!("test{}.png", i))));
        }
    }
    
    #[tokio::test]
    async fn test_process_directory_empty() {
        let temp_dir = TempDir::new().unwrap();
        
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let config = DefaultProcessingConfig::default();
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let summary = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(summary.total_files, 0);
        assert_eq!(summary.processed_files, 0);
        assert_eq!(summary.error_count, 0);
        assert!(persistence.is_finalized());
    }
    
    #[tokio::test]
    async fn test_process_directory_with_errors() {
        let temp_dir = TempDir::new().unwrap();
        
        // æœ‰åŠ¹ãªç”»åƒ
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        fs::write(temp_dir.path().join("valid.png"), &png_data).unwrap();
        fs::write(temp_dir.path().join("invalid.jpg"), b"not a valid image").unwrap();
        
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let config = DefaultProcessingConfig::default();
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let summary = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(summary.total_files, 2);
        assert_eq!(summary.processed_files, 1); // æœ‰åŠ¹ãªç”»åƒã®ã¿
        assert_eq!(summary.error_count, 1); // ç„¡åŠ¹ãªç”»åƒ
        
        let stored_data = persistence.get_stored_data();
        assert_eq!(stored_data.len(), 1);
        assert!(stored_data.contains_key(&format!("{}/valid.png", temp_dir.path().to_str().unwrap())));
    }
    
    #[tokio::test]
    async fn test_process_directory_performance_metrics() {
        let png_data = [
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
            0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
            0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
            0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
            0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
        ];
        
        let temp_dir = TempDir::new().unwrap();
        
        // 5ã¤ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        for i in 0..5 {
            let test_file = temp_dir.path().join(format!("test{}.png", i));
            fs::write(&test_file, &png_data).unwrap();
        }
        
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let config = DefaultProcessingConfig::default().with_max_concurrent(4);
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let start_time = std::time::Instant::now();
        
        let summary = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        let actual_elapsed = start_time.elapsed().as_millis() as u64;
        
        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
        assert_eq!(summary.processed_files, 5);
        assert!(summary.total_processing_time_ms > 0);
        assert!(summary.average_time_per_file_ms > 0.0);
        
        // å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã¨ã®å·®ãŒå¤§ãããªã„ã“ã¨ã‚’ç¢ºèªï¼ˆèª¤å·®ç¯„å›²ï¼‰
        let time_diff = if actual_elapsed > summary.total_processing_time_ms {
            actual_elapsed - summary.total_processing_time_ms
        } else {
            summary.total_processing_time_ms - actual_elapsed
        };
        assert!(time_diff < 1000); // 1ç§’ä»¥å†…ã®èª¤å·®
        
        // å¹³å‡æ™‚é–“ã®æ•´åˆæ€§ç¢ºèª
        let expected_avg = summary.total_processing_time_ms as f64 / 5.0;
        assert!((summary.average_time_per_file_ms - expected_avg).abs() < 1.0);
    }
}
```

**æˆåŠŸåŸºæº–**:
- `ParallelProcessor` ãƒˆãƒ¬ã‚¤ãƒˆå®Ÿè£…ã®å®Œäº†
- å®Œå…¨ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã®ç¢ºèª

---

#### **Task 5.2: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/error.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use thiserror::Error;

/// ä¸¦åˆ—å‡¦ç†å›ºæœ‰ã®ã‚¨ãƒ©ãƒ¼å‹
#[derive(Error, Debug)]
pub enum ProcessingError {
    #[error("ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ã‚¨ãƒ©ãƒ¼: {path} - {source}")]
    FileDiscoveryError {
        path: String,
        source: anyhow::Error,
    },
    
    #[error("ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {message}")]
    ParallelExecutionError {
        message: String,
    },
    
    #[error("æ°¸ç¶šåŒ–ã‚¨ãƒ©ãƒ¼: {source}")]
    PersistenceError {
        source: anyhow::Error,
    },
    
    #[error("è¨­å®šã‚¨ãƒ©ãƒ¼: {message}")]
    ConfigurationError {
        message: String,
    },
    
    #[error("ãƒãƒ£ãƒ³ãƒãƒ«ã‚¨ãƒ©ãƒ¼: {message}")]
    ChannelError {
        message: String,
    },
    
    #[error("ã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {source}")]
    TaskError {
        source: tokio::task::JoinError,
    },
}

impl ProcessingError {
    pub fn file_discovery(path: impl Into<String>, source: anyhow::Error) -> Self {
        Self::FileDiscoveryError {
            path: path.into(),
            source,
        }
    }
    
    pub fn parallel_execution(message: impl Into<String>) -> Self {
        Self::ParallelExecutionError {
            message: message.into(),
        }
    }
    
    pub fn persistence(source: anyhow::Error) -> Self {
        Self::PersistenceError { source }
    }
    
    pub fn configuration(message: impl Into<String>) -> Self {
        Self::ConfigurationError {
            message: message.into(),
        }
    }
    
    pub fn channel(message: impl Into<String>) -> Self {
        Self::ChannelError {
            message: message.into(),
        }
    }
    
    pub fn task(source: tokio::task::JoinError) -> Self {
        Self::TaskError { source }
    }
}

/// ä¸¦åˆ—å‡¦ç†ã®çµæœå‹
pub type ProcessingResult<T> = std::result::Result<T, ProcessingError>;
```

**engine.rs ã®æ›´æ–°**:
```rust
use super::error::{ProcessingError, ProcessingResult};

impl<L, H, S> ParallelProcessingEngine<L, H, S> {
    async fn discover_image_files(&self, path: &str) -> ProcessingResult<Vec<String>> {
        self.storage.list_items(path).await
            .map_err(|e| ProcessingError::file_discovery(path, e))
            .map(|items| {
                let mut image_files = Vec::new();
                for item in items {
                    if !item.is_directory && self.storage.is_image_file(&item) {
                        image_files.push(item.id);
                    }
                }
                image_files.sort();
                image_files
            })
    }
}

#[async_trait]
impl<L, H, S> ParallelProcessor for ParallelProcessingEngine<L, H, S> {
    async fn process_directory(
        &self,
        path: &str,
        config: &Self::Config,
        reporter: &Self::Reporter,
        persistence: &Self::Persistence,
    ) -> ProcessingResult<ProcessingSummary> {
        // ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if config.max_concurrent_tasks() == 0 {
            return Err(ProcessingError::configuration("ä¸¦åˆ—ã‚¿ã‚¹ã‚¯æ•°ã¯1ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"));
        }
        
        if config.batch_size() == 0 {
            return Err(ProcessingError::configuration("ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯1ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"));
        }
        
        let start_time = Instant::now();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹
        let files = self.discover_image_files(path).await?;
        let total_files = files.len();
        
        if config.enable_progress_reporting() {
            reporter.report_started(total_files).await;
        }
        
        // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        let pipeline = ProcessingPipeline::new(
            self.loader.clone(),
            self.hasher.clone(),
        );
        
        let mut summary = pipeline.execute(files, config, reporter, persistence).await
            .map_err(|e| ProcessingError::parallel_execution(format!("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {}", e)))?;
        
        // ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆæ¸¬
        let total_time = start_time.elapsed().as_millis() as u64;
        summary.total_processing_time_ms = total_time;
        
        if summary.processed_files > 0 {
            summary.average_time_per_file_ms = total_time as f64 / summary.processed_files as f64;
        }
        
        if config.enable_progress_reporting() {
            reporter.report_completed(summary.processed_files, summary.error_count).await;
        }
        
        // æ°¸ç¶šåŒ–å®Œäº†
        persistence.finalize().await
            .map_err(ProcessingError::persistence)?;
        
        Ok(summary)
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_processing_error_creation() {
        let file_error = ProcessingError::file_discovery(
            "/test/path",
            anyhow::anyhow!("ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"),
        );
        assert!(file_error.to_string().contains("/test/path"));
        assert!(file_error.to_string().contains("ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ã‚¨ãƒ©ãƒ¼"));
        
        let parallel_error = ProcessingError::parallel_execution("ä¸¦åˆ—å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ");
        assert!(parallel_error.to_string().contains("ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼"));
        
        let config_error = ProcessingError::configuration("ç„¡åŠ¹ãªè¨­å®šã§ã™");
        assert!(config_error.to_string().contains("è¨­å®šã‚¨ãƒ©ãƒ¼"));
    }
    
    #[tokio::test]
    async fn test_process_directory_validation_errors() {
        let temp_dir = TempDir::new().unwrap();
        
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        // ç„¡åŠ¹ãªä¸¦åˆ—æ•°
        let invalid_config = DefaultProcessingConfig::default().with_max_concurrent(0);
        let result = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &invalid_config,
            &reporter,
            &persistence,
        ).await;
        
        assert!(matches!(result, Err(ProcessingError::ConfigurationError { .. })));
        
        // ç„¡åŠ¹ãªãƒãƒƒãƒã‚µã‚¤ã‚º
        let invalid_config = DefaultProcessingConfig::default().with_batch_size(0);
        let result = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &invalid_config,
            &reporter,
            &persistence,
        ).await;
        
        assert!(matches!(result, Err(ProcessingError::ConfigurationError { .. })));
    }
    
    #[tokio::test]
    async fn test_process_nonexistent_directory() {
        let engine = ParallelProcessingEngine::new(
            StandardImageLoader::new(),
            DCTHasher::new(8),
            LocalStorageBackend::new(),
        );
        
        let config = DefaultProcessingConfig::default();
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let result = engine.process_directory(
            "/nonexistent/directory",
            &config,
            &reporter,
            &persistence,
        ).await;
        
        assert!(matches!(result, Err(ProcessingError::FileDiscoveryError { .. })));
    }
}
```

**ä¾å­˜é–¢ä¿‚è¿½åŠ **: `thiserror = "1.0"` ã‚’ `[dependencies]` ã«è¿½åŠ 

**æˆåŠŸåŸºæº–**:
- ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ©ãƒ¼å‹ã®å®Ÿè£…å®Œäº†
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ã®ç¢ºèª
- ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

### **Phase 6: JSONæ°¸ç¶šåŒ–å®Ÿè£…**
*æ¨å®šä½œæ¥­æ™‚é–“: 1æ—¥*

#### **Task 6.1: JsonHashPersistenceå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/implementations.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use serde::{Deserialize, Serialize};
use std::path::Path;
use tokio::fs::{File, OpenOptions};
use tokio::io::{AsyncWriteExt, BufWriter};

/// JSONå½¢å¼ã§ä¿å­˜ã™ã‚‹ãƒãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HashEntry {
    pub file_path: String,
    pub hash: String,
    pub metadata: ProcessingMetadata,
}

/// JSONå½¢å¼ã§ã®æ°¸ç¶šåŒ–å®Ÿè£…
#[derive(Debug)]
pub struct JsonHashPersistence {
    file_path: String,
    writer: Option<BufWriter<File>>,
    entries_written: usize,
}

impl JsonHashPersistence {
    pub fn new<P: AsRef<Path>>(file_path: P) -> Self {
        Self {
            file_path: file_path.as_ref().to_string_lossy().to_string(),
            writer: None,
            entries_written: 0,
        }
    }
    
    /// ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆJSONé…åˆ—é–‹å§‹ï¼‰
    async fn initialize_file(&mut self) -> Result<()> {
        if self.writer.is_some() {
            return Ok(());
        }
        
        // è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        if let Some(parent) = Path::new(&self.file_path).parent() {
            tokio::fs::create_dir_all(parent).await
                .map_err(|e| anyhow::anyhow!("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {}", e))?;
        }
        
        let file = OpenOptions::new()
            .create(true)
            .write(true)
            .truncate(true)
            .open(&self.file_path)
            .await
            .map_err(|e| anyhow::anyhow!("ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {}", e))?;
            
        let mut writer = BufWriter::new(file);
        
        // JSONé…åˆ—é–‹å§‹
        writer.write_all(b"[\n").await
            .map_err(|e| anyhow::anyhow!("æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {}", e))?;
            
        self.writer = Some(writer);
        Ok(())
    }
}

#[async_trait]
impl HashPersistence for JsonHashPersistence {
    async fn store_hash(
        &self,
        file_path: &str,
        hash: &str,
        metadata: &ProcessingMetadata,
    ) -> Result<()> {
        let entry = HashEntry {
            file_path: file_path.to_string(),
            hash: hash.to_string(),
            metadata: metadata.clone(),
        };
        
        self.store_batch(&[(entry.file_path, entry.hash, entry.metadata)]).await
    }
    
    async fn store_batch(
        &mut self,
        results: &[(String, String, ProcessingMetadata)],
    ) -> Result<()> {
        if results.is_empty() {
            return Ok(());
        }
        
        self.initialize_file().await?;
        
        let writer = self.writer.as_mut()
            .ok_or_else(|| anyhow::anyhow!("ãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"))?;
        
        for (file_path, hash, metadata) in results {
            let entry = HashEntry {
                file_path: file_path.clone(),
                hash: hash.clone(),
                metadata: metadata.clone(),
            };
            
            // ã‚«ãƒ³ãƒè¿½åŠ ï¼ˆæœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªä»¥å¤–ï¼‰
            if self.entries_written > 0 {
                writer.write_all(b",\n").await
                    .map_err(|e| anyhow::anyhow!("æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {}", e))?;
            }
            
            // JSON ã‚¨ãƒ³ãƒˆãƒªã‚’æ›¸ãè¾¼ã¿
            let json_str = serde_json::to_string_pretty(&entry)
                .map_err(|e| anyhow::anyhow!("JSONå¤‰æ›ã‚¨ãƒ©ãƒ¼: {}", e))?;
                
            // ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆè¿½åŠ 
            let indented = json_str.lines()
                .map(|line| format!("  {}", line))
                .collect::<Vec<_>>()
                .join("\n");
                
            writer.write_all(indented.as_bytes()).await
                .map_err(|e| anyhow::anyhow!("æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {}", e))?;
                
            self.entries_written += 1;
        }
        
        writer.flush().await
            .map_err(|e| anyhow::anyhow!("ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {}", e))?;
            
        Ok(())
    }
    
    async fn finalize(&mut self) -> Result<()> {
        if let Some(mut writer) = self.writer.take() {
            // JSONé…åˆ—çµ‚äº†
            writer.write_all(b"\n]").await
                .map_err(|e| anyhow::anyhow!("æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {}", e))?;
                
            writer.flush().await
                .map_err(|e| anyhow::anyhow!("ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {}", e))?;
        }
        
        Ok(())
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use tokio::fs;
    use serde_json::Value;

    #[tokio::test]
    async fn test_json_hash_persistence_single_entry() {
        let temp_dir = TempDir::new().unwrap();
        let json_file = temp_dir.path().join("test_hashes.json");
        
        let mut persistence = JsonHashPersistence::new(&json_file);
        
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 150,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        // å˜ä¸€ã‚¨ãƒ³ãƒˆãƒªä¿å­˜
        persistence.store_hash("/test.jpg", "abcd1234", &metadata).await.unwrap();
        
        // å®Œäº†å‡¦ç†
        persistence.finalize().await.unwrap();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª
        let content = fs::read_to_string(&json_file).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        
        assert!(json_value.is_array());
        let array = json_value.as_array().unwrap();
        assert_eq!(array.len(), 1);
        
        let entry = &array[0];
        assert_eq!(entry["file_path"], "/test.jpg");
        assert_eq!(entry["hash"], "abcd1234");
        assert_eq!(entry["metadata"]["file_size"], 1024);
        assert_eq!(entry["metadata"]["processing_time_ms"], 150);
    }
    
    #[tokio::test]
    async fn test_json_hash_persistence_batch() {
        let temp_dir = TempDir::new().unwrap();
        let json_file = temp_dir.path().join("batch_hashes.json");
        
        let mut persistence = JsonHashPersistence::new(&json_file);
        
        let metadata = ProcessingMetadata {
            file_size: 2048,
            processing_time_ms: 200,
            image_dimensions: (1024, 1024),
            was_resized: true,
        };
        
        // ãƒãƒƒãƒä¿å­˜
        let batch = vec![
            ("/test1.jpg".to_string(), "hash1".to_string(), metadata.clone()),
            ("/test2.png".to_string(), "hash2".to_string(), metadata.clone()),
            ("/test3.gif".to_string(), "hash3".to_string(), metadata.clone()),
        ];
        
        persistence.store_batch(&batch).await.unwrap();
        persistence.finalize().await.unwrap();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª
        let content = fs::read_to_string(&json_file).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        
        assert!(json_value.is_array());
        let array = json_value.as_array().unwrap();
        assert_eq!(array.len(), 3);
        
        for i in 0..3 {
            let entry = &array[i];
            assert_eq!(entry["file_path"], format!("/test{}.{}", i + 1, 
                match i { 0 => "jpg", 1 => "png", 2 => "gif", _ => unreachable!() }));
            assert_eq!(entry["hash"], format!("hash{}", i + 1));
            assert_eq!(entry["metadata"]["was_resized"], true);
        }
    }
    
    #[tokio::test]
    async fn test_json_hash_persistence_multiple_batches() {
        let temp_dir = TempDir::new().unwrap();
        let json_file = temp_dir.path().join("multi_batch.json");
        
        let mut persistence = JsonHashPersistence::new(&json_file);
        
        let metadata = ProcessingMetadata {
            file_size: 512,
            processing_time_ms: 100,
            image_dimensions: (256, 256),
            was_resized: false,
        };
        
        // è¤‡æ•°ãƒãƒƒãƒä¿å­˜
        let batch1 = vec![
            ("/batch1_1.jpg".to_string(), "hash1_1".to_string(), metadata.clone()),
            ("/batch1_2.jpg".to_string(), "hash1_2".to_string(), metadata.clone()),
        ];
        
        let batch2 = vec![
            ("/batch2_1.jpg".to_string(), "hash2_1".to_string(), metadata.clone()),
            ("/batch2_2.jpg".to_string(), "hash2_2".to_string(), metadata.clone()),
            ("/batch2_3.jpg".to_string(), "hash2_3".to_string(), metadata.clone()),
        ];
        
        persistence.store_batch(&batch1).await.unwrap();
        persistence.store_batch(&batch2).await.unwrap();
        persistence.finalize().await.unwrap();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª
        let content = fs::read_to_string(&json_file).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        
        assert!(json_value.is_array());
        let array = json_value.as_array().unwrap();
        assert_eq!(array.len(), 5);
        
        // é †åºç¢ºèª
        assert_eq!(array[0]["file_path"], "/batch1_1.jpg");
        assert_eq!(array[1]["file_path"], "/batch1_2.jpg");
        assert_eq!(array[2]["file_path"], "/batch2_1.jpg");
        assert_eq!(array[3]["file_path"], "/batch2_2.jpg");
        assert_eq!(array[4]["file_path"], "/batch2_3.jpg");
    }
    
    #[tokio::test]
    async fn test_json_hash_persistence_empty() {
        let temp_dir = TempDir::new().unwrap();
        let json_file = temp_dir.path().join("empty.json");
        
        let mut persistence = JsonHashPersistence::new(&json_file);
        
        // ä½•ã‚‚ä¿å­˜ã›ãšã«å®Œäº†
        persistence.finalize().await.unwrap();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        let content = fs::read_to_string(&json_file).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        
        assert!(json_value.is_array());
        let array = json_value.as_array().unwrap();
        assert_eq!(array.len(), 0);
    }
    
    #[tokio::test]
    async fn test_json_hash_persistence_directory_creation() {
        let temp_dir = TempDir::new().unwrap();
        let nested_path = temp_dir.path().join("nested").join("directory").join("hashes.json");
        
        let mut persistence = JsonHashPersistence::new(&nested_path);
        
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        persistence.store_hash("/test.jpg", "hash", &metadata).await.unwrap();
        persistence.finalize().await.unwrap();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert!(nested_path.exists());
        
        let content = fs::read_to_string(&nested_path).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        assert_eq!(json_value.as_array().unwrap().len(), 1);
    }
}
```

**ä¾å­˜é–¢ä¿‚è¿½åŠ **: `serde = { version = "1.0", features = ["derive"] }`, `serde_json = "1.0"` ã‚’ `[dependencies]` ã«è¿½åŠ 

**æˆåŠŸåŸºæº–**:
- JSONå½¢å¼ã§ã®æ°¸ç¶šåŒ–å®Ÿè£…å®Œäº†
- å˜ä¸€ãƒ»ãƒãƒƒãƒãƒ»è¤‡æ•°ãƒãƒƒãƒä¿å­˜ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªå‹•ä½œæˆã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

#### **Task 6.2: å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/implementations.rs`

**å®Ÿè£…å†…å®¹**:
```rust
/// å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œã®æ”¹è‰¯ç‰ˆJSONæ°¸ç¶šåŒ–
#[derive(Debug)]
pub struct StreamingJsonHashPersistence {
    file_path: String,
    writer: Option<BufWriter<File>>,
    entries_written: usize,
    buffer_size: usize,
    write_buffer: Vec<u8>,
}

impl StreamingJsonHashPersistence {
    pub fn new<P: AsRef<Path>>(file_path: P) -> Self {
        Self::with_buffer_size(file_path, 8192) // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ8KB
    }
    
    pub fn with_buffer_size<P: AsRef<Path>>(file_path: P, buffer_size: usize) -> Self {
        Self {
            file_path: file_path.as_ref().to_string_lossy().to_string(),
            writer: None,
            entries_written: 0,
            buffer_size,
            write_buffer: Vec::with_capacity(buffer_size),
        }
    }
    
    /// ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
    async fn flush_buffer(&mut self) -> Result<()> {
        if self.write_buffer.is_empty() {
            return Ok(());
        }
        
        if let Some(writer) = &mut self.writer {
            writer.write_all(&self.write_buffer).await
                .map_err(|e| anyhow::anyhow!("ãƒãƒƒãƒ•ã‚¡æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {}", e))?;
            writer.flush().await
                .map_err(|e| anyhow::anyhow!("ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {}", e))?;
        }
        
        self.write_buffer.clear();
        Ok(())
    }
    
    /// ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼‰
    async fn write_to_buffer(&mut self, data: &[u8]) -> Result<()> {
        self.write_buffer.extend_from_slice(data);
        
        if self.write_buffer.len() >= self.buffer_size {
            self.flush_buffer().await?;
        }
        
        Ok(())
    }
}

#[async_trait]
impl HashPersistence for StreamingJsonHashPersistence {
    async fn store_hash(
        &mut self,
        file_path: &str,
        hash: &str,
        metadata: &ProcessingMetadata,
    ) -> Result<()> {
        self.store_batch(&[(file_path.to_string(), hash.to_string(), metadata.clone())]).await
    }
    
    async fn store_batch(
        &mut self,
        results: &[(String, String, ProcessingMetadata)],
    ) -> Result<()> {
        if results.is_empty() {
            return Ok(());
        }
        
        // å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–
        if self.writer.is_none() {
            if let Some(parent) = Path::new(&self.file_path).parent() {
                tokio::fs::create_dir_all(parent).await
                    .map_err(|e| anyhow::anyhow!("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {}", e))?;
            }
            
            let file = OpenOptions::new()
                .create(true)
                .write(true)
                .truncate(true)
                .open(&self.file_path)
                .await
                .map_err(|e| anyhow::anyhow!("ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {}", e))?;
                
            self.writer = Some(BufWriter::with_capacity(self.buffer_size * 2, file));
            self.write_to_buffer(b"[\n").await?;
        }
        
        for (file_path, hash, metadata) in results {
            let entry = HashEntry {
                file_path: file_path.clone(),
                hash: hash.clone(),
                metadata: metadata.clone(),
            };
            
            // ã‚«ãƒ³ãƒè¿½åŠ ï¼ˆæœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªä»¥å¤–ï¼‰
            if self.entries_written > 0 {
                self.write_to_buffer(b",\n").await?;
            }
            
            // ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªJSONç”Ÿæˆï¼ˆpretty printãªã—ï¼‰
            let json_str = serde_json::to_string(&entry)
                .map_err(|e| anyhow::anyhow!("JSONå¤‰æ›ã‚¨ãƒ©ãƒ¼: {}", e))?;
            
            // ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆè¿½åŠ ï¼ˆæœ€å°é™ï¼‰
            let indented = format!("  {}", json_str);
            self.write_to_buffer(indented.as_bytes()).await?;
            
            self.entries_written += 1;
        }
        
        Ok(())
    }
    
    async fn finalize(&mut self) -> Result<()> {
        // æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
        self.flush_buffer().await?;
        
        if let Some(mut writer) = self.writer.take() {
            writer.write_all(b"\n]").await
                .map_err(|e| anyhow::anyhow!("æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {}", e))?;
            writer.flush().await
                .map_err(|e| anyhow::anyhow!("æœ€çµ‚ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {}", e))?;
        }
        
        Ok(())
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_streaming_json_persistence_large_dataset() {
        let temp_dir = TempDir::new().unwrap();
        let json_file = temp_dir.path().join("large_dataset.json");
        
        let mut persistence = StreamingJsonHashPersistence::with_buffer_size(&json_file, 1024); // å°ã•ãªãƒãƒƒãƒ•ã‚¡
        
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        // å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒã§ä¿å­˜
        let batch_size = 100;
        let total_entries = 1000;
        
        for batch_idx in 0..(total_entries / batch_size) {
            let mut batch = Vec::new();
            for i in 0..batch_size {
                let entry_idx = batch_idx * batch_size + i;
                batch.push((
                    format!("/test{}.jpg", entry_idx),
                    format!("hash{}", entry_idx),
                    metadata.clone(),
                ));
            }
            
            persistence.store_batch(&batch).await.unwrap();
        }
        
        persistence.finalize().await.unwrap();
        
        // ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç¢ºèª
        let content = fs::read_to_string(&json_file).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        
        assert!(json_value.is_array());
        let array = json_value.as_array().unwrap();
        assert_eq!(array.len(), total_entries);
        
        // ã„ãã¤ã‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ç¢ºèª
        assert_eq!(array[0]["file_path"], "/test0.jpg");
        assert_eq!(array[0]["hash"], "hash0");
        assert_eq!(array[999]["file_path"], "/test999.jpg");
        assert_eq!(array[999]["hash"], "hash999");
    }
    
    #[tokio::test]
    async fn test_streaming_vs_regular_performance() {
        let temp_dir = TempDir::new().unwrap();
        
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        let test_data: Vec<_> = (0..500).map(|i| (
            format!("/test{}.jpg", i),
            format!("hash{}", i),
            metadata.clone(),
        )).collect();
        
        // é€šå¸¸ç‰ˆã®ãƒ†ã‚¹ãƒˆ
        let regular_file = temp_dir.path().join("regular.json");
        let start_regular = std::time::Instant::now();
        {
            let mut regular = JsonHashPersistence::new(&regular_file);
            regular.store_batch(&test_data).await.unwrap();
            regular.finalize().await.unwrap();
        }
        let regular_time = start_regular.elapsed();
        
        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆã®ãƒ†ã‚¹ãƒˆ
        let streaming_file = temp_dir.path().join("streaming.json");
        let start_streaming = std::time::Instant::now();
        {
            let mut streaming = StreamingJsonHashPersistence::new(&streaming_file);
            streaming.store_batch(&test_data).await.unwrap();
            streaming.finalize().await.unwrap();
        }
        let streaming_time = start_streaming.elapsed();
        
        // çµæœã®å†…å®¹ãŒåŒã˜ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        let regular_content = fs::read_to_string(&regular_file).await.unwrap();
        let streaming_content = fs::read_to_string(&streaming_file).await.unwrap();
        
        let regular_json: Value = serde_json::from_str(&regular_content).unwrap();
        let streaming_json: Value = serde_json::from_str(&streaming_content).unwrap();
        
        assert_eq!(regular_json.as_array().unwrap().len(), 500);
        assert_eq!(streaming_json.as_array().unwrap().len(), 500);
        
        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆãƒ†ã‚¹ãƒˆãƒ­ã‚°ç”¨ï¼‰
        println!("Regular: {:?}, Streaming: {:?}", regular_time, streaming_time);
        
        // ã©ã¡ã‚‰ã‚‚åˆç†çš„ãªæ™‚é–“ã§å®Œäº†ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert!(regular_time.as_millis() < 5000); // 5ç§’ä»¥å†…
        assert!(streaming_time.as_millis() < 5000); // 5ç§’ä»¥å†…
    }
    
    #[tokio::test]
    async fn test_streaming_memory_efficiency() {
        let temp_dir = TempDir::new().unwrap();
        let json_file = temp_dir.path().join("memory_test.json");
        
        // å°ã•ãªãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã§å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        let mut persistence = StreamingJsonHashPersistence::with_buffer_size(&json_file, 256);
        
        let metadata = ProcessingMetadata {
            file_size: 1024,
            processing_time_ms: 100,
            image_dimensions: (512, 512),
            was_resized: false,
        };
        
        // å°ã•ãªãƒãƒƒãƒã‚’å¤§é‡ã«é€ä¿¡ã—ã¦ãƒãƒƒãƒ•ã‚¡ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚’ãƒ†ã‚¹ãƒˆ
        for i in 0..100 {
            let batch = vec![(
                format!("/test{}.jpg", i),
                format!("very_long_hash_string_to_test_buffer_efficiency_{}", i),
                metadata.clone(),
            )];
            
            persistence.store_batch(&batch).await.unwrap();
        }
        
        persistence.finalize().await.unwrap();
        
        let content = fs::read_to_string(&json_file).await.unwrap();
        let json_value: Value = serde_json::from_str(&content).unwrap();
        
        assert_eq!(json_value.as_array().unwrap().len(), 100);
        
        // ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆç†çš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        let file_metadata = fs::metadata(&json_file).await.unwrap();
        assert!(file_metadata.len() > 1000); // æœ€å°ã‚µã‚¤ã‚ºç¢ºèª
        assert!(file_metadata.len() < 1024 * 1024); // 1MBä»¥ä¸‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    }
}
```

**æˆåŠŸåŸºæº–**:
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ°¸ç¶šåŒ–ã®å®Ÿè£…å®Œäº†
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ã®ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

### **Phase 7: çµ±åˆãƒ†ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
*æ¨å®šä½œæ¥­æ™‚é–“: 0.5æ—¥*

#### **Task 7.1: çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/integration_tests.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use image_dedup::{
    App,
    image_loader::standard::StandardImageLoader,
    perceptual_hash::dct_hash::DCTHasher,
    storage::local::LocalStorageBackend,
    processing::{
        ParallelProcessingEngine,
        DefaultProcessingConfig,
        ConsoleProgressReporter,
        StreamingJsonHashPersistence,
        ParallelProcessor,
    },
};
use tempfile::TempDir;
use std::fs;
use serde_json::Value;

/// ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªPNGç”»åƒãƒ‡ãƒ¼ã‚¿
const SMALL_PNG: &[u8] = &[
    0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
    0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
    0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
    0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
    0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
    0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
];

/// ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
fn create_test_directory_structure(temp_dir: &TempDir, file_count: usize) {
    let base_path = temp_dir.path();
    
    // ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    for i in 0..file_count {
        let file_path = base_path.join(format!("image_{:03}.png", i));
        fs::write(&file_path, SMALL_PNG).unwrap();
    }
    
    // ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ç”»åƒä½œæˆ
    let sub_dir = base_path.join("subdir");
    fs::create_dir(&sub_dir).unwrap();
    
    for i in 0..(file_count / 2) {
        let file_path = sub_dir.join(format!("sub_image_{:03}.png", i));
        fs::write(&file_path, SMALL_PNG).unwrap();
    }
    
    // éç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆç„¡è¦–ã•ã‚Œã‚‹ã¹ãï¼‰
    fs::write(base_path.join("readme.txt"), b"This is not an image").unwrap();
    fs::write(base_path.join("data.json"), br#"{"key": "value"}"#).unwrap();
    
    // ç„¡åŠ¹ãªç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    fs::write(base_path.join("invalid.jpg"), b"This is not a valid image").unwrap();
}

#[tokio::test]
async fn test_end_to_end_processing_small_dataset() {
    let temp_dir = TempDir::new().unwrap();
    create_test_directory_structure(&temp_dir, 10);
    
    // å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
    let engine = ParallelProcessingEngine::new(
        StandardImageLoader::new(),
        DCTHasher::new(8),
        LocalStorageBackend::new(),
    );
    
    // è¨­å®š
    let config = DefaultProcessingConfig::default()
        .with_max_concurrent(4)
        .with_batch_size(3)
        .with_progress_reporting(false); // ãƒ†ã‚¹ãƒˆç”¨ã«ç„¡åŠ¹åŒ–
    
    let reporter = ConsoleProgressReporter::quiet();
    
    // JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
    let output_file = temp_dir.path().join("hashes_output.json");
    let persistence = StreamingJsonHashPersistence::new(&output_file);
    
    // å‡¦ç†å®Ÿè¡Œ
    let summary = engine.process_directory(
        temp_dir.path().to_str().unwrap(),
        &config,
        &reporter,
        &persistence,
    ).await.unwrap();
    
    // çµæœæ¤œè¨¼
    assert_eq!(summary.total_files, 16); // 10 + 5(sub) + 1(invalid) = 16
    assert_eq!(summary.processed_files, 15); // æœ‰åŠ¹ãªç”»åƒã®ã¿
    assert_eq!(summary.error_count, 1); // ç„¡åŠ¹ãªç”»åƒ1ã¤
    assert!(summary.total_processing_time_ms > 0);
    assert!(summary.average_time_per_file_ms > 0.0);
    
    // JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    assert!(output_file.exists());
    let content = fs::read_to_string(&output_file).unwrap();
    let json: Value = serde_json::from_str(&content).unwrap();
    
    assert!(json.is_array());
    let entries = json.as_array().unwrap();
    assert_eq!(entries.len(), 15);
    
    // å„ã‚¨ãƒ³ãƒˆãƒªã®æ§‹é€ ç¢ºèª
    for entry in entries {
        assert!(entry["file_path"].is_string());
        assert!(entry["hash"].is_string());
        assert!(entry["metadata"]["file_size"].is_number());
        assert!(entry["metadata"]["processing_time_ms"].is_number());
        assert!(entry["metadata"]["image_dimensions"].is_array());
        assert!(entry["metadata"]["was_resized"].is_boolean());
    }
}

#[tokio::test]
async fn test_end_to_end_processing_medium_dataset() {
    let temp_dir = TempDir::new().unwrap();
    create_test_directory_structure(&temp_dir, 100);
    
    let engine = ParallelProcessingEngine::new(
        StandardImageLoader::new(),
        DCTHasher::new(8),
        LocalStorageBackend::new(),
    );
    
    let config = DefaultProcessingConfig::default()
        .with_max_concurrent(8)
        .with_batch_size(20)
        .with_progress_reporting(false);
    
    let reporter = ConsoleProgressReporter::quiet();
    let output_file = temp_dir.path().join("medium_dataset.json");
    let persistence = StreamingJsonHashPersistence::new(&output_file);
    
    let start_time = std::time::Instant::now();
    
    let summary = engine.process_directory(
        temp_dir.path().to_str().unwrap(),
        &config,
        &reporter,
        &persistence,
    ).await.unwrap();
    
    let elapsed = start_time.elapsed();
    
    // çµæœæ¤œè¨¼
    assert_eq!(summary.total_files, 151); // 100 + 50(sub) + 1(invalid) = 151
    assert_eq!(summary.processed_files, 150);
    assert_eq!(summary.error_count, 1);
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèªï¼ˆä¸­ç¨‹åº¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
    assert!(elapsed.as_secs() < 30); // 30ç§’ä»¥å†…ã§å®Œäº†
    assert!(summary.average_time_per_file_ms < 200.0); // 1ãƒ•ã‚¡ã‚¤ãƒ«200msä»¥ä¸‹
    
    // JSONç¢ºèª
    let content = fs::read_to_string(&output_file).unwrap();
    let json: Value = serde_json::from_str(&content).unwrap();
    assert_eq!(json.as_array().unwrap().len(), 150);
}

#[tokio::test] 
async fn test_concurrent_processing_stress() {
    let temp_dir = TempDir::new().unwrap();
    create_test_directory_structure(&temp_dir, 50);
    
    // åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å¯¾ã—ã¦è¤‡æ•°ã®å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åŒæ™‚å®Ÿè¡Œ
    // ï¼ˆå®Ÿéš›ã®ä½¿ç”¨ä¾‹ã§ã¯éæ¨å¥¨ã ãŒã€ä¸¦è¡Œå‡¦ç†ã®å …ç‰¢æ€§ã‚’ãƒ†ã‚¹ãƒˆï¼‰
    
    let mut handles = Vec::new();
    
    for i in 0..3 {
        let temp_path = temp_dir.path().to_str().unwrap().to_string();
        let output_file = temp_dir.path().join(format!("concurrent_{}.json", i));
        
        let handle = tokio::spawn(async move {
            let engine = ParallelProcessingEngine::new(
                StandardImageLoader::new(),
                DCTHasher::new(8),
                LocalStorageBackend::new(),
            );
            
            let config = DefaultProcessingConfig::default()
                .with_max_concurrent(4)
                .with_batch_size(5)
                .with_progress_reporting(false);
            
            let reporter = ConsoleProgressReporter::quiet();
            let persistence = StreamingJsonHashPersistence::new(&output_file);
            
            engine.process_directory(&temp_path, &config, &reporter, &persistence).await
        });
        
        handles.push(handle);
    }
    
    // å…¨ã‚¿ã‚¹ã‚¯å®Œäº†å¾…æ©Ÿ
    let mut results = Vec::new();
    for handle in handles {
        let result = handle.await.unwrap().unwrap();
        results.push(result);
    }
    
    // å…¨ã¦ã®çµæœãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    for summary in &results {
        assert_eq!(summary.total_files, 76); // 50 + 25(sub) + 1(invalid)
        assert_eq!(summary.processed_files, 75);
        assert_eq!(summary.error_count, 1);
    }
    
    // å…¨ã¦ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    for i in 0..3 {
        let output_file = temp_dir.path().join(format!("concurrent_{}.json", i));
        assert!(output_file.exists());
        
        let content = fs::read_to_string(&output_file).unwrap();
        let json: Value = serde_json::from_str(&content).unwrap();
        assert_eq!(json.as_array().unwrap().len(), 75);
    }
}

#[tokio::test]
async fn test_error_recovery_and_partial_processing() {
    let temp_dir = TempDir::new().unwrap();
    let base_path = temp_dir.path();
    
    // æœ‰åŠ¹ãªç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    for i in 0..5 {
        let file_path = base_path.join(format!("valid_{}.png", i));
        fs::write(&file_path, SMALL_PNG).unwrap();
    }
    
    // è¤‡æ•°ã®ç„¡åŠ¹ãªç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    for i in 0..5 {
        let file_path = base_path.join(format!("invalid_{}.jpg", i));
        fs::write(&file_path, format!("Invalid image data {}", i).as_bytes()).unwrap();
    }
    
    // æ¨©é™ã‚¨ãƒ©ãƒ¼ã‚’å¼•ãèµ·ã“ã™ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆUnixã‚·ã‚¹ãƒ†ãƒ ã®ã¿ï¼‰
    #[cfg(unix)]
    {
        let restricted_file = base_path.join("restricted.png");
        fs::write(&restricted_file, SMALL_PNG).unwrap();
        use std::os::unix::fs::PermissionsExt;
        let mut perms = fs::metadata(&restricted_file).unwrap().permissions();
        perms.set_mode(0o000); // èª­ã¿å–ã‚Šæ¨©é™ãªã—
        fs::set_permissions(&restricted_file, perms).unwrap();
    }
    
    let engine = ParallelProcessingEngine::new(
        StandardImageLoader::new(),
        DCTHasher::new(8),
        LocalStorageBackend::new(),
    );
    
    let config = DefaultProcessingConfig::default()
        .with_max_concurrent(3)
        .with_batch_size(2)
        .with_progress_reporting(false);
    
    let reporter = ConsoleProgressReporter::quiet();
    let output_file = temp_dir.path().join("error_recovery.json");
    let persistence = StreamingJsonHashPersistence::new(&output_file);
    
    let summary = engine.process_directory(
        temp_dir.path().to_str().unwrap(),
        &config,
        &reporter,
        &persistence,
    ).await.unwrap();
    
    // ã‚¨ãƒ©ãƒ¼å›å¾©ç¢ºèª
    assert_eq!(summary.processed_files, 5); // æœ‰åŠ¹ãªç”»åƒã®ã¿
    
    #[cfg(unix)]
    assert!(summary.error_count >= 6); // 5ã¤ã®ç„¡åŠ¹ + 1ã¤ã®æ¨©é™ã‚¨ãƒ©ãƒ¼
    
    #[cfg(not(unix))]
    assert_eq!(summary.error_count, 5); // 5ã¤ã®ç„¡åŠ¹ãªç”»åƒ
    
    // æœ‰åŠ¹ãªçµæœã¯æ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    let content = fs::read_to_string(&output_file).unwrap();
    let json: Value = serde_json::from_str(&content).unwrap();
    assert_eq!(json.as_array().unwrap().len(), 5);
}

#[tokio::test]
async fn test_empty_directory_handling() {
    let temp_dir = TempDir::new().unwrap();
    
    // ç©ºã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿ä½œæˆ
    fs::create_dir(temp_dir.path().join("empty_sub")).unwrap();
    
    let engine = ParallelProcessingEngine::new(
        StandardImageLoader::new(),
        DCTHasher::new(8),
        LocalStorageBackend::new(),
    );
    
    let config = DefaultProcessingConfig::default();
    let reporter = ConsoleProgressReporter::quiet();
    let output_file = temp_dir.path().join("empty_result.json");
    let persistence = StreamingJsonHashPersistence::new(&output_file);
    
    let summary = engine.process_directory(
        temp_dir.path().to_str().unwrap(),
        &config,
        &reporter,
        &persistence,
    ).await.unwrap();
    
    // ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‡¦ç†ç¢ºèª
    assert_eq!(summary.total_files, 0);
    assert_eq!(summary.processed_files, 0);
    assert_eq!(summary.error_count, 0);
    assert_eq!(summary.total_processing_time_ms, 0);
    
    // ç©ºã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    let content = fs::read_to_string(&output_file).unwrap();
    let json: Value = serde_json::from_str(&content).unwrap();
    assert!(json.is_array());
    assert_eq!(json.as_array().unwrap().len(), 0);
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
- ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆå°ãƒ»ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
- ä¸¦è¡Œå‡¦ç†ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
- ã‚¨ãƒ©ãƒ¼å›å¾©ã¨éƒ¨åˆ†å‡¦ç†ãƒ†ã‚¹ãƒˆ
- ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

**æˆåŠŸåŸºæº–**:
- å…¨çµ±åˆãƒ†ã‚¹ãƒˆã®ãƒ‘ã‚¹
- å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã®å‹•ä½œç¢ºèª
- ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å …ç‰¢æ€§ç¢ºèª

---

#### **Task 7.2: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `benches/parallel_processing_bench.rs`

**å®Ÿè£…å†…å®¹**:
```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use image_dedup::{
    image_loader::standard::StandardImageLoader,
    perceptual_hash::dct_hash::DCTHasher,
    storage::local::LocalStorageBackend,
    processing::{
        ParallelProcessingEngine,
        DefaultProcessingConfig,
        NoOpProgressReporter,
        MemoryHashPersistence,
        ParallelProcessor,
    },
};
use tempfile::TempDir;
use std::fs;
use tokio::runtime::Runtime;

/// ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªPNGç”»åƒãƒ‡ãƒ¼ã‚¿
const SMALL_PNG: &[u8] = &[
    0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
    0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
    0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
    0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
    0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
    0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
];

/// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
fn create_test_files(temp_dir: &TempDir, count: usize) {
    let base_path = temp_dir.path();
    
    for i in 0..count {
        let file_path = base_path.join(format!("bench_image_{:04}.png", i));
        fs::write(&file_path, SMALL_PNG).unwrap();
    }
}

/// ä¸¦åˆ—åº¦åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
fn bench_concurrency_scaling(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let temp_dir = TempDir::new().unwrap();
    create_test_files(&temp_dir, 100);
    
    let mut group = c.benchmark_group("concurrency_scaling");
    
    for concurrency in [1, 2, 4, 8, 16].iter() {
        group.bench_with_input(
            BenchmarkId::new("parallel_processing", concurrency), 
            concurrency,
            |b, &concurrency| {
                b.to_async(&rt).iter(|| async {
                    let engine = ParallelProcessingEngine::new(
                        StandardImageLoader::new(),
                        DCTHasher::new(8),
                        LocalStorageBackend::new(),
                    );
                    
                    let config = DefaultProcessingConfig::default()
                        .with_max_concurrent(concurrency)
                        .with_batch_size(10)
                        .with_progress_reporting(false);
                    
                    let reporter = NoOpProgressReporter::new();
                    let persistence = MemoryHashPersistence::new();
                    
                    black_box(
                        engine.process_directory(
                            temp_dir.path().to_str().unwrap(),
                            &config,
                            &reporter,
                            &persistence,
                        ).await.unwrap()
                    );
                });
            }
        );
    }
    group.finish();
}

/// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
fn bench_dataset_scaling(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    let mut group = c.benchmark_group("dataset_scaling");
    group.sample_size(10); // å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãŸã‚ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¸›ã‚‰ã™
    
    for file_count in [10, 50, 100, 200].iter() {
        let temp_dir = TempDir::new().unwrap();
        create_test_files(&temp_dir, *file_count);
        
        group.bench_with_input(
            BenchmarkId::new("file_count", file_count),
            file_count,
            |b, &file_count| {
                b.to_async(&rt).iter(|| async {
                    let engine = ParallelProcessingEngine::new(
                        StandardImageLoader::new(),
                        DCTHasher::new(8),
                        LocalStorageBackend::new(),
                    );
                    
                    let config = DefaultProcessingConfig::default()
                        .with_max_concurrent(4)
                        .with_batch_size(20)
                        .with_progress_reporting(false);
                    
                    let reporter = NoOpProgressReporter::new();
                    let persistence = MemoryHashPersistence::new();
                    
                    black_box(
                        engine.process_directory(
                            temp_dir.path().to_str().unwrap(),
                            &config,
                            &reporter,
                            &persistence,
                        ).await.unwrap()
                    );
                });
            }
        );
    }
    group.finish();
}

/// ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
fn bench_batch_size_optimization(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let temp_dir = TempDir::new().unwrap();
    create_test_files(&temp_dir, 100);
    
    let mut group = c.benchmark_group("batch_size_optimization");
    
    for batch_size in [1, 5, 10, 20, 50].iter() {
        group.bench_with_input(
            BenchmarkId::new("batch_size", batch_size),
            batch_size,
            |b, &batch_size| {
                b.to_async(&rt).iter(|| async {
                    let engine = ParallelProcessingEngine::new(
                        StandardImageLoader::new(),
                        DCTHasher::new(8),
                        LocalStorageBackend::new(),
                    );
                    
                    let config = DefaultProcessingConfig::default()
                        .with_max_concurrent(4)
                        .with_batch_size(batch_size)
                        .with_progress_reporting(false);
                    
                    let reporter = NoOpProgressReporter::new();
                    let persistence = MemoryHashPersistence::new();
                    
                    black_box(
                        engine.process_directory(
                            temp_dir.path().to_str().unwrap(),
                            &config,
                            &reporter,
                            &persistence,
                        ).await.unwrap()
                    );
                });
            }
        );
    }
    group.finish();
}

/// ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
fn bench_memory_efficiency(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let temp_dir = TempDir::new().unwrap();
    create_test_files(&temp_dir, 50);
    
    let mut group = c.benchmark_group("memory_efficiency");
    
    // ãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã®å½±éŸ¿
    for buffer_size in [10, 50, 100, 200].iter() {
        group.bench_with_input(
            BenchmarkId::new("channel_buffer", buffer_size),
            buffer_size,
            |b, &buffer_size| {
                b.to_async(&rt).iter(|| async {
                    let engine = ParallelProcessingEngine::new(
                        StandardImageLoader::new(),
                        DCTHasher::new(8),
                        LocalStorageBackend::new(),
                    );
                    
                    let config = DefaultProcessingConfig::default()
                        .with_max_concurrent(4)
                        .with_buffer_size(buffer_size)
                        .with_batch_size(10)
                        .with_progress_reporting(false);
                    
                    let reporter = NoOpProgressReporter::new();
                    let persistence = MemoryHashPersistence::new();
                    
                    black_box(
                        engine.process_directory(
                            temp_dir.path().to_str().unwrap(),
                            &config,
                            &reporter,
                            &persistence,
                        ).await.unwrap()
                    );
                });
            }
        );
    }
    group.finish();
}

criterion_group!(
    benches,
    bench_concurrency_scaling,
    bench_dataset_scaling,
    bench_batch_size_optimization,
    bench_memory_efficiency
);
criterion_main!(benches);
```

**Cargo.toml ã¸ã®è¿½åŠ **:
```toml
[[bench]]
name = "parallel_processing_bench"
harness = false

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports", "async_tokio"] }
```

**æˆåŠŸåŸºæº–**:
- ä¸¦åˆ—åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ¸¬å®šå®Œäº†
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºå½±éŸ¿ã®æ¸¬å®šå®Œäº†
- æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã®ç‰¹å®š

---

### **Phase 8: å…¬é–‹APIæ•´å‚™**
*æ¨å®šä½œæ¥­æ™‚é–“: 0.5æ—¥*

#### **Task 8.1: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…¬é–‹è¨­å®š**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/lib.rs`

**å®Ÿè£…å†…å®¹**:
```rust
//! # ç”»åƒé‡è¤‡æ¤œå‡ºãƒ„ãƒ¼ãƒ« - ä¸¦åˆ—å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
//! 
//! ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€å¤§é‡ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŠ¹ç‡çš„ã«ä¸¦åˆ—å‡¦ç†ã—ã€
//! çŸ¥è¦šãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®é«˜æ€§èƒ½ãªä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚
//! 
//! ## ä¸»ãªæ©Ÿèƒ½
//! 
//! - **é«˜æ€§èƒ½ãªä¸¦åˆ—å‡¦ç†**: Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªä¸¦åˆ—å®Ÿè¡Œ
//! - **æŸ”è»Ÿãªè¨­å®š**: ä¸¦åˆ—åº¦ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã®èª¿æ•´å¯èƒ½
//! - **è¤‡æ•°ã®æ°¸ç¶šåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³**: JSONã€ãƒ¡ãƒ¢ãƒªå†…ä¿å­˜ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ
//! - **åŒ…æ‹¬çš„ãªé€²æ—å ±å‘Š**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã¨ã‚¨ãƒ©ãƒ¼å ±å‘Š
//! - **å³å¯†ãªå‹å®‰å…¨æ€§**: Rustã®å‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ´»ç”¨ã—ãŸã‚¨ãƒ©ãƒ¼é˜²æ­¢
//! 
//! ## åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹
//! 
//! ```rust
//! use image_dedup::{
//!     App,
//!     image_loader::standard::StandardImageLoader,
//!     perceptual_hash::dct_hash::DCTHasher,
//!     storage::local::LocalStorageBackend,
//!     processing::{
//!         ParallelProcessingEngine,
//!         DefaultProcessingConfig,
//!         ConsoleProgressReporter,
//!         StreamingJsonHashPersistence,
//!         ParallelProcessor,
//!     },
//! };
//! 
//! #[tokio::main]
//! async fn main() -> Result<(), Box<dyn std::error::Error>> {
//!     // 1. ä¾å­˜é–¢ä¿‚ã®æ§‹ç¯‰
//!     let loader = StandardImageLoader::new();
//!     let hasher = DCTHasher::new(8);
//!     let storage = LocalStorageBackend::new();
//!     
//!     // 2. ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰
//!     let engine = ParallelProcessingEngine::new(loader, hasher, storage);
//!     
//!     // 3. è¨­å®šã¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æº–å‚™
//!     let config = DefaultProcessingConfig::default()
//!         .with_max_concurrent(8)
//!         .with_batch_size(50);
//!     
//!     let reporter = ConsoleProgressReporter::new();
//!     let persistence = StreamingJsonHashPersistence::new("hashes.json");
//!     
//!     // 4. å‡¦ç†å®Ÿè¡Œ
//!     let summary = engine.process_directory(
//!         "./images",
//!         &config,
//!         &reporter,
//!         &persistence,
//!     ).await?;
//!     
//!     println!("å‡¦ç†å®Œäº†: {}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†, {}ã‚¨ãƒ©ãƒ¼, {}ms",
//!              summary.processed_files,
//!              summary.error_count,
//!              summary.total_processing_time_ms);
//!     
//!     Ok(())
//! }
//! ```
//! 
//! ## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
//! 
//! ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ä»¥ä¸‹ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™:
//! 
//! - **[`processing`]**: ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã‚³ã‚¢æŠ½è±¡åŒ–
//! - **[`image_loader`]**: ç”»åƒèª­ã¿è¾¼ã¿æŠ½è±¡åŒ–
//! - **[`perceptual_hash`]**: çŸ¥è¦šãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
//! - **[`storage`]**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ“ä½œæŠ½è±¡åŒ–

pub mod image_loader;
pub mod perceptual_hash;
pub mod storage;

// ä¸¦åˆ—å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å…¬é–‹
pub mod processing;

// ä¾¿åˆ©ãªå†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
pub use processing::{
    // ä¸»è¦ãªãƒˆãƒ¬ã‚¤ãƒˆ
    ParallelProcessor,
    ProcessingConfig,
    ProgressReporter,
    HashPersistence,
    
    // ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    ProcessingSummary,
    ProcessingMetadata,
    
    // å…·è±¡å®Ÿè£…
    ParallelProcessingEngine,
    DefaultProcessingConfig,
    ConsoleProgressReporter,
    NoOpProgressReporter,
    MemoryHashPersistence,
    JsonHashPersistence,
    StreamingJsonHashPersistence,
    
    // ã‚¨ãƒ©ãƒ¼å‹
    ProcessingError,
};

// DIã‚³ãƒ³ãƒ†ãƒŠã®å½¹å‰²ã‚’æœãŸã™ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ãªAppæ§‹é€ ä½“
pub struct App<L, H, S>
where
    L: image_loader::ImageLoaderBackend,
    H: perceptual_hash::PerceptualHashBackend,
    S: storage::StorageBackend,
{
    pub loader: L,
    pub hasher: H,
    pub storage: S,
}

impl<L, H, S> App<L, H, S>
where
    L: image_loader::ImageLoaderBackend,
    H: Perceptual HashBackend,
    S: storage::StorageBackend,
{
    /// æ–°ã—ã„Appã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
    pub fn new(loader: L, hasher: H, storage: S) -> Self {
        Self {
            loader,
            hasher,
            storage,
        }
    }

    /// ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã«å¤‰æ›
    /// 
    /// # ä¾‹
    /// 
    /// ```rust
    /// use image_dedup::*;
    /// 
    /// let app = App::new(
    ///     image_loader::standard::StandardImageLoader::new(),
    ///     perceptual_hash::dct_hash::DCTHasher::new(8),
    ///     storage::local::LocalStorageBackend::new(),
    /// );
    /// 
    /// let engine = app.into_parallel_engine();
    /// ```
    pub fn into_parallel_engine(self) -> ParallelProcessingEngine<L, H, S>
    where
        L: 'static,
        H: 'static,
        S: 'static,
    {
        ParallelProcessingEngine::from_app(self)
    }

    /// ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸»è¦ãªãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œï¼ˆå¾“æ¥ã®åŒæœŸå‡¦ç†ï¼‰
    /// 
    /// **æ³¨æ„**: ã“ã®é–¢æ•°ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã•ã‚Œã¦ãŠã‚Šã€
    /// æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ `into_parallel_engine()` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
    pub async fn run(&self, path: &str) -> anyhow::Result<()> {
        println!("Starting image deduplication process in: {path}");

        let items = self.storage.list_items(path).await?;
        let image_files = items.iter().filter(|item| self.storage.is_image_file(item));

        for item in image_files {
            println!("Processing: {}", item.name);
            // ã“ã“ã§ç”»åƒã®èª­ã¿è¾¼ã¿ã€ãƒãƒƒã‚·ãƒ¥åŒ–ã€æ¯”è¼ƒãªã©ã®å‡¦ç†ã‚’å®Ÿè£…
            // let image_data = self.storage.read_item(&item.id).await?;
            // let loaded_image = self.loader.load_from_bytes(&image_data).await?;
            // let hash = self.hasher.generate_hash(&loaded_image.image).await?;
            // println!("  - Hash: {}", hash.to_hex());
        }

        println!("Process finished.");
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::image_loader::standard::StandardImageLoader;
    use crate::perceptual_hash::average_hash::AverageHasher;
    use crate::storage::{MockStorageBackend, StorageItem};
    use mockall::predicate::*;

    #[tokio::test]
    async fn test_app_creation() {
        let app = App::new(
            StandardImageLoader::new(),
            AverageHasher::new(8),
            storage::local::LocalStorageBackend::new(),
        );
        
        // ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆãƒ†ã‚¹ãƒˆ
        assert!(true);
    }

    #[tokio::test]
    async fn test_app_to_parallel_engine_conversion() {
        let app = App::new(
            StandardImageLoader::new(),
            perceptual_hash::dct_hash::DCTHasher::new(8),
            storage::local::LocalStorageBackend::new(),
        );
        
        let engine = app.into_parallel_engine();
        
        // å¤‰æ›ãƒ†ã‚¹ãƒˆ
        assert!(true);
    }

    #[tokio::test]
    async fn test_run_with_mock_storage() {
        let mut mock_storage = MockStorageBackend::new();

        // `list_items`ãŒå‘¼ã°ã‚ŒãŸã¨ãã®æŒ¯ã‚‹èˆã„ã‚’å®šç¾©
        mock_storage
            .expect_list_items()
            .with(eq("test_path"))
            .times(1)
            .returning(|_| {
                Ok(vec![
                    StorageItem {
                        id: "image1.jpg".to_string(),
                        name: "image1.jpg".to_string(),
                        size: 1024,
                        is_directory: false,
                        extension: Some("jpg".to_string()),
                    },
                    StorageItem {
                        id: "not_an_image.txt".to_string(),
                        name: "not_an_image.txt".to_string(),
                        size: 100,
                        is_directory: false,
                        extension: Some("txt".to_string()),
                    },
                ])
            });

        // `is_image_file`ãŒå‘¼ã°ã‚ŒãŸã¨ãã®æŒ¯ã‚‹èˆã„ã‚’å®šç¾©
        mock_storage
            .expect_is_image_file()
            .returning(|item| matches!(item.extension.as_deref(), Some("jpg")));

        let app = App::new(
            StandardImageLoader::new(),
            AverageHasher::new(8),
            mock_storage,
        );

        let result = app.run("test_path").await;
        assert!(result.is_ok());
    }
}
```

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```rust
#[cfg(test)]
mod integration_tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;
    
    const SMALL_PNG: &[u8] = &[
        0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A, 0x00, 0x00, 0x00, 0x0D,
        0x49, 0x48, 0x44, 0x52, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
        0x08, 0x06, 0x00, 0x00, 0x00, 0x1F, 0x15, 0xC4, 0x89, 0x00, 0x00, 0x00,
        0x0A, 0x49, 0x44, 0x41, 0x54, 0x78, 0x9C, 0x63, 0x00, 0x01, 0x00, 0x00,
        0x05, 0x00, 0x01, 0x0D, 0x0A, 0x2D, 0xB4, 0x00, 0x00, 0x00, 0x00, 0x49,
        0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82,
    ];

    #[tokio::test]
    async fn test_public_api_usage() {
        let temp_dir = TempDir::new().unwrap();
        
        // ãƒ†ã‚¹ãƒˆç”»åƒä½œæˆ
        for i in 0..5 {
            let file_path = temp_dir.path().join(format!("test_{}.png", i));
            fs::write(&file_path, SMALL_PNG).unwrap();
        }
        
        // å…¬é–‹APIã‚’ä½¿ã£ãŸå‡¦ç†
        let engine = ParallelProcessingEngine::new(
            image_loader::standard::StandardImageLoader::new(),
            perceptual_hash::dct_hash::DCTHasher::new(8),
            storage::local::LocalStorageBackend::new(),
        );
        
        let config = DefaultProcessingConfig::default()
            .with_max_concurrent(2)
            .with_batch_size(3);
        
        let reporter = ConsoleProgressReporter::quiet();
        let persistence = MemoryHashPersistence::new();
        
        let summary = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(summary.processed_files, 5);
        assert_eq!(summary.error_count, 0);
    }

    #[tokio::test]
    async fn test_app_to_engine_conversion() {
        let temp_dir = TempDir::new().unwrap();
        fs::write(temp_dir.path().join("test.png"), SMALL_PNG).unwrap();
        
        let app = App::new(
            image_loader::standard::StandardImageLoader::new(),
            perceptual_hash::dct_hash::DCTHasher::new(8),
            storage::local::LocalStorageBackend::new(),
        );
        
        let engine = app.into_parallel_engine();
        
        let config = DefaultProcessingConfig::default();
        let reporter = NoOpProgressReporter::new();
        let persistence = MemoryHashPersistence::new();
        
        let summary = engine.process_directory(
            temp_dir.path().to_str().unwrap(),
            &config,
            &reporter,
            &persistence,
        ).await.unwrap();
        
        assert_eq!(summary.processed_files, 1);
        assert_eq!(summary.error_count, 0);
    }
}
```

**æˆåŠŸåŸºæº–**:
- å…¬é–‹APIã®é©åˆ‡ãªéœ²å‡º
- å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®å‹•ä½œç¢ºèª
- å¾Œæ–¹äº’æ›æ€§ã®ç¶­æŒ

---

#### **Task 8.2: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**
**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/processing/mod.rs`

**å®Ÿè£…å†…å®¹**:
```rust
//! # ä¸¦åˆ—å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
//! 
//! ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€å¤§é‡ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŠ¹ç‡çš„ã«ä¸¦åˆ—å‡¦ç†ã™ã‚‹ãŸã‚ã®ã‚³ã‚¢æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
//! Producer-Consumerãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ä¿¡é ¼æ€§ã®é«˜ã„ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
//! 
//! ## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦
//! 
//! ```text
//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
//! â”‚    Producer     â”‚â”€â”€â”€â–¶â”‚  Consumer Pool   â”‚â”€â”€â”€â–¶â”‚ Result Collectorâ”‚
//! â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
//! â”‚ ãƒ»File Discoveryâ”‚    â”‚ ãƒ»Image Loading  â”‚    â”‚ ãƒ»Batch Storage â”‚
//! â”‚ ãƒ»Path Queuing  â”‚    â”‚ ãƒ»Hash Generationâ”‚    â”‚ ãƒ»Progress Trackâ”‚
//! â”‚                 â”‚    â”‚ ãƒ»Error Handling â”‚    â”‚ ãƒ»Summary Reportâ”‚
//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//! ```
//! 
//! ## ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
//! 
//! ### ãƒˆãƒ¬ã‚¤ãƒˆï¼ˆæŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰
//! 
//! - [`ParallelProcessor`]: ä¸¦åˆ—å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
//! - [`ProcessingConfig`]: å‡¦ç†è¨­å®šã®æŠ½è±¡åŒ–  
//! - [`ProgressReporter`]: é€²æ—å ±å‘Šã®æŠ½è±¡åŒ–
//! - [`HashPersistence`]: çµæœæ°¸ç¶šåŒ–ã®æŠ½è±¡åŒ–
//! 
//! ### å…·è±¡å®Ÿè£…
//! 
//! - [`ParallelProcessingEngine`]: ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
//! - [`DefaultProcessingConfig`]: æ¨™æº–è¨­å®šå®Ÿè£…
//! - [`ConsoleProgressReporter`]: ã‚³ãƒ³ã‚½ãƒ¼ãƒ«é€²æ—å ±å‘Š
//! - [`JsonHashPersistence`]: JSONå½¢å¼æ°¸ç¶šåŒ–
//! - [`StreamingJsonHashPersistence`]: å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œJSONæ°¸ç¶šåŒ–
//! 
//! ## ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
//! 
//! ### åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹
//! 
//! ```rust
//! use image_dedup::processing::*;
//! 
//! # async fn example() -> Result<(), Box<dyn std::error::Error>> {
//! // ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
//! let engine = ParallelProcessingEngine::new(
//!     /* loader */, /* hasher */, /* storage */
//! );
//! 
//! // è¨­å®š
//! let config = DefaultProcessingConfig::default()
//!     .with_max_concurrent(8)
//!     .with_batch_size(50);
//! 
//! // å®Ÿè¡Œ
//! let summary = engine.process_directory(
//!     "/path/to/images",
//!     &config,
//!     &ConsoleProgressReporter::new(),
//!     &StreamingJsonHashPersistence::new("output.json"),
//! ).await?;
//! 
//! println!("å‡¦ç†å®Œäº†: {}", summary.processed_files);
//! # Ok(())
//! # }
//! ```
//! 
//! ### é«˜åº¦ãªè¨­å®šä¾‹
//! 
//! ```rust
//! # use image_dedup::processing::*;
//! # async fn advanced_example() -> Result<(), Box<dyn std::error::Error>> {
//! // ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
//! let config = DefaultProcessingConfig::default()
//!     .with_max_concurrent(16)      // é«˜ã„ä¸¦åˆ—åº¦
//!     .with_buffer_size(200)        // å¤§ããªãƒãƒƒãƒ•ã‚¡  
//!     .with_batch_size(100)         // å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
//!     .with_progress_reporting(true);
//! 
//! // ã‚«ã‚¹ã‚¿ãƒ é€²æ—å ±å‘Šï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆï¼‰
//! let reporter = NoOpProgressReporter::new();
//! 
//! // ãƒ¡ãƒ¢ãƒªå†…ä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
//! let persistence = MemoryHashPersistence::new();
//! 
//! // å‡¦ç†å®Ÿè¡Œ
//! // ...
//! # Ok(())
//! # }
//! ```
//! 
//! ## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´
//! 
//! ### ä¸¦åˆ—åº¦ã®èª¿æ•´
//! 
//! ```rust
//! # use image_dedup::processing::*;
//! // CPUé›†ç´„çš„: ã‚³ã‚¢æ•°ã¨åŒã˜
//! let cpu_bound = DefaultProcessingConfig::default()
//!     .with_max_concurrent(num_cpus::get());
//! 
//! // I/Oãƒã‚¦ãƒ³ãƒ‰: ã‚³ã‚¢æ•°ã®1.5-2å€
//! let io_bound = DefaultProcessingConfig::default()  
//!     .with_max_concurrent(num_cpus::get() * 2);
//! ```
//! 
//! ### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®åˆ¶å¾¡
//! 
//! ```rust
//! # use image_dedup::processing::*;
//! // å°ãƒ¡ãƒ¢ãƒªç’°å¢ƒç”¨
//! let low_memory = DefaultProcessingConfig::default()
//!     .with_buffer_size(50)     // å°ã•ãªãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ•ã‚¡
//!     .with_batch_size(10);     // å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º
//! 
//! // é«˜ãƒ¡ãƒ¢ãƒªç’°å¢ƒç”¨
//! let high_memory = DefaultProcessingConfig::default()
//!     .with_buffer_size(500)    // å¤§ããªãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ•ã‚¡  
//!     .with_batch_size(200);    // å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
//! ```
//! 
//! ## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
//! 
//! ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’æä¾›ã—ã¾ã™ï¼š
//! 
//! ```rust
//! # use image_dedup::processing::*;
//! # async fn error_handling_example() -> Result<(), ProcessingError> {
//! match engine.process_directory("/path", &config, &reporter, &persistence).await {
//!     Ok(summary) => {
//!         println!("æˆåŠŸ: {}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†", summary.processed_files);
//!         if summary.error_count > 0 {
//!             println!("è­¦å‘Š: {}ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼", summary.error_count);
//!         }
//!     }
//!     Err(ProcessingError::FileDiscoveryError { path, source }) => {
//!         eprintln!("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {} - {}", path, source);
//!     }
//!     Err(ProcessingError::ConfigurationError { message }) => {
//!         eprintln!("è¨­å®šã‚¨ãƒ©ãƒ¼: {}", message);  
//!     }
//!     Err(e) => {
//!         eprintln!("ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼: {}", e);
//!     }
//! }
//! # Ok(())
//! # }
//! ```
//! 
//! ## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§
//! 
//! - **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 1000ãƒ•ã‚¡ã‚¤ãƒ«/åˆ† (å…¸å‹çš„ãªç’°å¢ƒ)
//! - **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: ãƒ•ã‚¡ã‚¤ãƒ«å½“ãŸã‚Š50-200ms
//! - **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: è¨­å®šå¯èƒ½ãªãƒãƒƒãƒ•ã‚¡ã«ã‚ˆã‚‹åˆ¶å¾¡
//! - **CPUä½¿ç”¨ç‡**: ä¸¦åˆ—åº¦ã«ã‚ˆã‚‹ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
//! 
//! ## åˆ¶é™äº‹é …
//! 
//! - æœ€å¤§ä¸¦åˆ—åº¦: 1000ï¼ˆå®Ÿç”¨çš„ã«ã¯32ç¨‹åº¦ã‚’æ¨å¥¨ï¼‰
//! - æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º: 10000ï¼ˆå®Ÿç”¨çš„ã«ã¯100ç¨‹åº¦ã‚’æ¨å¥¨ï¼‰
//! - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã«æ¯”ä¾‹

// ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…å®¹ï¼ˆå®Ÿéš›ã®å®Ÿè£…ï¼‰
// ... (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰)
```

**å„ãƒˆãƒ¬ã‚¤ãƒˆã¨æ§‹é€ ä½“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ **:
```rust
/// ä¸¦åˆ—å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®æŠ½è±¡åŒ–
/// 
/// ã“ã®ãƒˆãƒ¬ã‚¤ãƒˆã¯ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†ã™ã‚‹ãŸã‚ã®
/// é«˜ãƒ¬ãƒ™ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚å®Ÿè£…ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ã€
/// ä¸¦åˆ—ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆã€çµæœåé›†ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚
/// 
/// # å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
/// 
/// - `Config`: å‡¦ç†è¨­å®šã®å‹
/// - `Reporter`: é€²æ—å ±å‘Šã®å‹  
/// - `Persistence`: çµæœæ°¸ç¶šåŒ–ã®å‹
/// 
/// # ä¾‹
/// 
/// ```rust
/// # use image_dedup::processing::*;
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// let processor: ParallelProcessingEngine<_, _, _> = /* ... */;
/// let config = DefaultProcessingConfig::default();
/// let reporter = ConsoleProgressReporter::new();
/// let persistence = JsonHashPersistence::new("results.json");
/// 
/// let summary = processor.process_directory(
///     "/images",
///     &config, 
///     &reporter,
///     &persistence
/// ).await?;
/// # Ok(())
/// # }
/// ```
#[async_trait]
pub trait ParallelProcessor: Send + Sync {
    // ... æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰
}

/// ä¸¦åˆ—å‡¦ç†è¨­å®šã®æŠ½è±¡åŒ–
/// 
/// å‡¦ç†ã®å‹•ä½œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã™ã‚‹ãƒˆãƒ¬ã‚¤ãƒˆã§ã™ã€‚
/// ä¸¦åˆ—åº¦ã€ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãªã©ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚
/// 
/// # å®Ÿè£…ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
/// 
/// - `max_concurrent_tasks`: 1ä»¥ä¸Šã®å€¤ã‚’è¿”ã™å¿…è¦ãŒã‚ã‚Šã¾ã™
/// - `channel_buffer_size`: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®
/// - `batch_size`: I/OåŠ¹ç‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
/// - `enable_progress_reporting`: falseã«ã™ã‚‹ã¨ã‚ãšã‹ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
pub trait ProcessingConfig: Send + Sync {
    // ... æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰
}
```

**æˆåŠŸåŸºæº–**:
- åŒ…æ‹¬çš„ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œæˆ
- `cargo doc` ã§ã®æ­£å¸¸ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
- ä½¿ç”¨ä¾‹ã®å‹•ä½œç¢ºèª

---

## å®Ÿè£…å®Œäº†å¾Œã®æ¤œè¨¼

### å…¨ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test

# ã‚¯ãƒªãƒƒãƒ—å®Ÿè¡Œ
cargo clippy

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
cargo doc --open

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
cargo bench

# çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test --test integration_tests
```

### æ¨å¥¨æœ€çµ‚ç¢ºèªé …ç›®

1. **æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ**: å„Phaseã®æ©Ÿèƒ½ãŒæ­£å¸¸å‹•ä½œ
2. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ**: æƒ³å®šã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®é”æˆ
3. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ**: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®ç„¡ã„ã“ã¨
4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ**: æ§˜ã€…ãªã‚¨ãƒ©ãƒ¼æ¡ä»¶ã§ã®å …ç‰¢æ€§
5. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ**: ä½¿ç”¨ä¾‹ãŒæ­£å¸¸ã«å‹•ä½œ

---

## ã¾ã¨ã‚

æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ã€ç”»åƒé‡è¤‡æ¤œå‡ºãƒ„ãƒ¼ãƒ«ã®ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã®è©³ç´°ãªã‚¿ã‚¹ã‚¯è¨ˆç”»æ›¸ã§ã™ã€‚Phase 1ã‹ã‚‰8ã¾ã§ã€åˆè¨ˆ**25ã®å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯**ã§æ§‹æˆã•ã‚Œã€å„ã‚¿ã‚¹ã‚¯ã«ã¯ï¼š

- **è©³ç´°ãªå®Ÿè£…ã‚³ãƒ¼ãƒ‰ä¾‹**
- **åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹**
- **æ˜ç¢ºãªæˆåŠŸåŸºæº–**
- **å¿…è¦ãªä¾å­˜é–¢ä¿‚æƒ…å ±**

å„ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã¯å¿…ãš `cargo test && cargo clippy` ã§å“è³ªã‚’ç¢ºèªã—ã€æ¬¡ã®ã‚¿ã‚¹ã‚¯ã«é€²ã‚€ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

**åˆè¨ˆæ¨å®šä½œæ¥­æ™‚é–“**: ç´„4.5æ—¥
**æ¨å¥¨å®Ÿè£…é †åº**: Phase 1 â†’ Phase 2 â†’ ... â†’ Phase 8

ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ãŒã“ã®ãƒ¬ãƒãƒ¼ãƒˆã«å¾“ã£ã¦ã€ä¸€è²«ã—ãŸå“è³ªã§æ®µéšçš„ã«ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’å®Ÿè£…ã§ãã¾ã™ã€‚